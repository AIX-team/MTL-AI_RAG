import time
import os
import requests
import datetime
import tiktoken
from cachetools import TTLCache
from math import ceil
from langdetect import detect
from dotenv import load_dotenv
from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound
from bs4 import BeautifulSoup
import openai
from googleapiclient.discovery import build
import googlemaps
from typing import List, Dict, Tuple, Any
from models.youtube_schemas import YouTubeResponse, VideoInfo, PlaceInfo, PlacePhoto, ContentType, ContentInfo, PlaceGeometry
from repository.youtube_repository import YouTubeRepository
from langchain.schema import Document
from urllib.parse import urlparse, parse_qs
from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from fastapi import APIRouter, HTTPException, status



from ai_api.youtube_subtitle import (
    get_video_info, process_link, split_text, summarize_text,
    extract_place_names, search_place_details, get_place_photo_google
)

# í™˜ê²½ ë³€ìˆ˜ ë° ìƒìˆ˜ ì„¤ì •
load_dotenv(dotenv_path=".env")

# API í‚¤ ì„¤ì •
openai.api_key = os.getenv("OPENAI_API_KEY")
GEOCODING_API_KEY = os.getenv("GOOGLE_GEOCODING_API_KEY")
GOOGLE_PLACES_API_KEY = os.getenv("GOOGLE_PLACES_API_KEY")

# ìƒìˆ˜ ì„¤ì •
MAX_URLS = 5
CHUNK_SIZE = 2048
MODEL = "gpt-4o-mini"
FINAL_SUMMARY_MAX_TOKENS = 1500

class ContentService:
    """ì»¨í…ì¸  ì²˜ë¦¬ ì„œë¹„ìŠ¤"""
    
    @staticmethod
    def get_content_info(url: str) -> Tuple[str, str, ContentType]:
        """URLì—ì„œ ì»¨í…ì¸  ì •ë³´ ì¶”ì¶œ"""
        content_type = ContentService._detect_content_type(url)
        
        if content_type == ContentType.YOUTUBE:
            title, author = YouTubeSubtitleService.get_video_info(url)
        elif content_type == ContentType.NAVER_BLOG:
            title, author = ContentService._get_naver_blog_info(url)
        elif content_type == ContentType.TISTORY:
            title, author = ContentService._get_tistory_blog_info(url)
        else:
            title, author = ContentService._get_webpage_info(url)
            
        return title, author, content_type

    @staticmethod
    def _detect_content_type(url: str) -> ContentType:
        """URL ìœ í˜• ê°ì§€"""
        domain = urlparse(url).netloc.lower()
        
        if "youtube.com" in domain or "youtu.be" in domain:
            return ContentType.YOUTUBE
        elif "blog.naver.com" in domain:
            return ContentType.NAVER_BLOG
        elif ".tistory.com" in domain:
            return ContentType.TISTORY
        elif url.endswith(".txt"):
            return ContentType.TEXT_FILE
        elif url.startswith("http"):
            return ContentType.WEBPAGE
        return ContentType.UNKNOWN

    @staticmethod
    def _get_naver_blog_info(url: str) -> Tuple[str, str]:
        """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì •ë³´ ì¶”ì¶œ"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(url, headers=headers)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # iframe ë‚´ë¶€ì˜ ì‹¤ì œ ì»¨í…ì¸  URL ì°¾ê¸°
            if 'blog.naver.com' in url:
                iframe = soup.find('iframe', id='mainFrame')
                if iframe and iframe.get('src'):
                    real_url = f"https://blog.naver.com{iframe['src']}"
                    response = requests.get(real_url, headers=headers)
                    soup = BeautifulSoup(response.text, 'html.parser')
            
            title = soup.find('meta', property='og:title')
            title = title['content'] if title else "ì œëª© ì—†ìŒ"
            
            author = soup.find('meta', property='og:article:author')
            author = author['content'] if author else "ì‘ì„±ì ì—†ìŒ"
            
            return title, author
        except Exception as e:
            print(f"ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None, None

    @staticmethod
    def _get_tistory_blog_info(url: str) -> Tuple[str, str]:
        """í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ ì •ë³´ ì¶”ì¶œ"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(url, headers=headers)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            title = soup.find('meta', property='og:title')
            title = title['content'] if title else "ì œëª© ì—†ìŒ"
            
            author = soup.find('meta', property='og:article:author')
            author = author['content'] if author else "ì‘ì„±ì ì—†ìŒ"
            
            return title, author
        except Exception as e:
            print(f"í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None, None

    @staticmethod
    def process_content(url: str) -> str:
        """URLì—ì„œ ì»¨í…ì¸  ì¶”ì¶œ"""
        content_type = ContentService._detect_content_type(url)
        
        if content_type == ContentType.YOUTUBE:
            return YouTubeSubtitleService.process_link(url)
        elif content_type == ContentType.NAVER_BLOG:
            return ContentService._get_naver_blog_content(url)
        elif content_type == ContentType.TISTORY:
            return ContentService._get_tistory_blog_content(url)
        elif content_type == ContentType.TEXT_FILE:
            return YouTubeSubtitleService._get_text_from_file(url)
        else:
            return YouTubeSubtitleService._get_text_from_webpage(url)

    @staticmethod
    def _get_naver_blog_content(url: str) -> str:
        """ë„¤ì´ë²„ ë¸”ë¡œê·¸ì—ì„œ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ (ë¶ˆí•„ìš”í•œ ê°œí–‰ ë° ê³µë°±, ê´‘ê³  ì œê±° í¬í•¨)"""
        cache = TTLCache(maxsize=10, ttl=300)

        if url in cache:
            return cache[url]

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }

        def clean_text(text: str) -> str:
            import re
            text = re.sub(r'\s+', ' ', text)  # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ ë³€í™˜
            text = re.sub(r'[^\S\r\n]+', ' ', text)  # ìœ ë‹ˆì½”ë“œ ê³µë°± ì œê±°
            text = re.sub(r'Â©.*?(?= )', '', text)   # Â© ë“± ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ ì œê±°
            text = re.sub(r'\[ë°”ë¡œê°€ê¸°\]', '', text)
            return text.strip()

        try:
            # ì²« ë²ˆì§¸ ìš”ì²­ìœ¼ë¡œ iframe URL ê°€ì ¸ì˜¤ê¸°
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            # iframe ì°¾ê¸° (ìƒˆ ë²„ì „ ë¸”ë¡œê·¸)
            iframe = soup.find('iframe', id='mainFrame')
            if iframe and iframe.get('src'):
                real_url = f"https://blog.naver.com{iframe['src']}"
                response = requests.get(real_url, headers=headers)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')

            # ìƒˆë¡œìš´ ë²„ì „ ë¸”ë¡œê·¸ ì˜ì—­
            content = soup.find('div', {'class': 'se-main-container'})
            if not content:
                # êµ¬ë²„ì „ ë¸”ë¡œê·¸ ì˜ì—­
                content = soup.find('div', {'class': 'post-view'})
            if not content:
                raise HTTPException(status_code=404, detail="ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
            for tag in content.find_all(['script', 'style']):
                tag.decompose()

            text = clean_text(content.get_text(separator='\n'))
            cache[url] = text
            return text

        except requests.RequestException as e:
            raise HTTPException(status_code=500, detail=f"ë¸”ë¡œê·¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    @staticmethod
    def _get_tistory_blog_content(url: str) -> str:
        """í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ ì»¨í…ì¸  ì¶”ì¶œ"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(url, headers=headers)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë³¸ë¬¸ ì»¨í…ì¸  ì°¾ê¸°
            content = soup.find('div', {'class': 'entry-content'})
            if not content:
                content = soup.find('div', {'class': 'article'})
            
            if content:
                # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                for element in content.find_all(['script', 'style']):
                    element.decompose()
                
                text = content.get_text(separator='\n').strip()
                return text
            
            return "ì»¨í…ì¸ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        except Exception as e:
            print(f"í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ ì»¨í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return "ì»¨í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨"

class YouTubeService:
    """ë©”ì¸ YouTube ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        """YouTubeService ì´ˆê¸°í™”"""
        from dotenv import load_dotenv
        load_dotenv()  # .env íŒŒì¼ ë¡œë“œ
        
        self.repository = YouTubeRepository()
        self.content_service = ContentService()
        self.text_service = TextProcessingService()
        self.place_service = PlaceService()
        
        # ì²­í¬ í¬ê¸° ì œí•œ ì„¤ì •
        self.MAX_CHUNK_SIZE = 50000  # 50KB
        self.MAX_TOTAL_SIZE = 200000  # 200KB
        
        # Google Maps API í‚¤ í™•ì¸ ë° ì„¤ì •
        google_maps_api_key = os.getenv('GOOGLE_PLACES_API_KEY')
        if not google_maps_api_key:
            raise ValueError("GOOGLE_PLACES_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            self.gmaps = googlemaps.Client(key=google_maps_api_key)
        except Exception as e:
            raise ValueError(f"Google Maps í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")

    async def process_urls(self, urls: List[str]) -> Dict:
        """URL ëª©ë¡ì„ ì²˜ë¦¬í•˜ì—¬ ê°ê°ì˜ ìš”ì•½ì„ ìƒì„±"""
        try:
            content_infos = []
            place_details = []
            final_summaries = {}  # ìµœì¢… ìš”ì•½ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
            start_time = time.time()

            for url in urls:
                # URL ì²˜ë¦¬ í¬ê¸° ì œí•œ í™•ì¸
                if len(url.encode()) > self.MAX_CHUNK_SIZE:
                    print(f"Warning: URL too long, skipping: {url[:100]}...")
                    continue

                parsed_url = urlparse(url)
                if 'youtube.com' in parsed_url.netloc:
                    await self._process_youtube_url(url, content_infos, place_details)
                elif 'blog.naver.com' in parsed_url.netloc:
                    await self._process_naver_blog_url(url, content_infos, place_details)

            processing_time = time.time() - start_time

            # ê²°ê³¼ ë°ì´í„° í¬ê¸° ì œí•œ
            result = self._create_limited_result(content_infos, place_details, processing_time)
                    
            await self.repository.save_to_vectordb(final_summaries, content_infos, place_details)
            
            return result

        except Exception as e:
            print(f"Error in process_urls: {str(e)}")
            raise ValueError(f"URL ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


    def _create_limited_result(self, content_infos, place_details, processing_time):
        """ê²°ê³¼ ë°ì´í„° í¬ê¸°ë¥¼ ì œí•œí•˜ì—¬ ìƒì„±"""
        summaries = {}
        limited_place_details = []
        
        total_size = 0
        
        # ì»¨í…ì¸  ì •ë³´ ì œí•œ
        for info in content_infos:
            if total_size >= self.MAX_TOTAL_SIZE:
                break
            
            summary = self._format_final_result(
                content_infos=[info],
                place_details=[p for p in place_details if p.source_url == info.url],
                processing_time=processing_time,
                urls=[info.url]
            )
            
            summary_size = len(str(summary).encode())
            if total_size + summary_size <= self.MAX_TOTAL_SIZE:
                summaries[info.url] = summary
                total_size += summary_size
        
        # ì¥ì†Œ ìƒì„¸ ì •ë³´ ì œí•œ
        for place in place_details:
            place_size = len(str(place.dict()).encode())
            if total_size + place_size <= self.MAX_TOTAL_SIZE:
                limited_place_details.append(place)
                total_size += place_size
            else:
                break
        
        return {
            "summary": summaries,
            "content_infos": [info.dict() for info in content_infos],
            "processing_time_seconds": processing_time,
            "place_details": [place.dict() for place in limited_place_details]
        }

    async def _process_youtube_url(self, url, content_infos, place_details):
        """YouTube URL ì²˜ë¦¬"""
        video_id = parse_qs(urlparse(url).query).get('v', [None])[0]
        if video_id:
            video_info = self._get_video_info(video_id)
            content_info = ContentInfo(
                url=url,
                title=video_info.title,
                author=video_info.channel,
                platform=ContentType.YOUTUBE
            )
            content_infos.append(content_info)
            
            video_places = self._process_youtube_video(video_id, url)
            place_details.extend(video_places)

    async def _process_naver_blog_url(self, url, content_infos, place_details):
        """ë„¤ì´ë²„ ë¸”ë¡œê·¸ URL ì²˜ë¦¬"""
        title, author = self.content_service._get_naver_blog_info(url)
        content = self.content_service.process_content(url)
        
        content_info = ContentInfo(
            url=url,
            title=title,
            author=author,
            platform=ContentType.NAVER_BLOG
        )
        content_infos.append(content_info)
        
        blog_places = self._process_naver_blog(url)
        place_details.extend(blog_places)

    def _process_youtube_video(self, video_id: str, source_url: str) -> List[PlaceInfo]:
        """YouTube ì˜ìƒì„ ì²˜ë¦¬í•˜ì—¬ ì¥ì†Œ ì •ë³´ë¥¼ ìˆ˜ì§‘"""
        try:
            # YouTube ìë§‰ ê°€ì ¸ì˜¤ê¸°
            transcript_text = self._get_youtube_transcript(video_id)
            
            # í…ìŠ¤íŠ¸ ë¶„í•  ë° ìš”ì•½
            chunks = self.text_service.split_text(transcript_text)
            summary = self.text_service.summarize_text(chunks)
            
            # ì¥ì†Œ ì¶”ì¶œ ë° ì •ë³´ ìˆ˜ì§‘
            place_names = self.place_service.extract_place_names(summary)
            print(f"ì¶”ì¶œëœ ì¥ì†Œ: {place_names}")
            
            # ì¥ì†Œ ì •ë³´ ìˆ˜ì§‘
            place_details = []
            for place_info in place_names:
                try:
                    # ì¥ì†Œëª…ê³¼ ì§€ì—­ëª… ë¶„ë¦¬
                    if " (" in place_info and ")" in place_info:
                        place_name, area = place_info.split(" (")
                        area = area.rstrip(")")
                    else:
                        place_name = place_info
                        area = "ì¼ë³¸"
                    
                    # Google Places APIë¡œ ì¥ì†Œ ì •ë³´ ê²€ìƒ‰
                    places_result = self.gmaps.places(f"{place_name} {area}")
                    if not places_result['results']:
                        print(f"ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: {place_name} {area}")
                        continue
                    
                    # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ ê²°ê³¼ë¡œ ì§„í–‰
                    place = places_result['results'][0]
                    place_id = place['place_id']
                    details = self.gmaps.place(place_id, language='ko')['result']
                    
                    # ì¥ì†Œ íƒ€ì…ê³¼ ì¢Œí‘œ ì •ë³´ ì¶”ì¶œ
                    place_type = details.get('types', ['unknown'])[0]
                    location = details.get('geometry', {}).get('location', {})
                    geometry = PlaceGeometry(
                        latitude=location.get('lat'),
                        longitude=location.get('lng')
                    )
                    
                    # ìƒˆë¡œìš´ PlaceInfo ê°ì²´ ìƒì„±
                    place_info_obj = PlaceInfo(
                        name=place_name,
                        source_url=source_url,
                        type=place_type,
                        geometry=geometry,
                        description=self._extract_place_description(summary, place_name),
                        official_description=self._get_place_description_from_openai(place_name, place_type),
                        formatted_address=details.get('formatted_address'),
                        rating=details.get('rating'),
                        phone=details.get('formatted_phone_number'),
                        website=details.get('website'),
                        price_level=details.get('price_level'),
                        opening_hours=details.get('opening_hours', {}).get('weekday_text'),
                        google_info=details
                    )
                    
                    # ì‚¬ì§„ URL ì¶”ê°€
                    if 'photos' in details:
                        photo_ref = details['photos'][0]['photo_reference']
                        photo_url = f"https://maps.googleapis.com/maps/api/place/photo?maxwidth=400&photoreference={photo_ref}&key={os.getenv('GOOGLE_PLACES_API_KEY')}"
                        place_info_obj.photos = [PlacePhoto(url=photo_url)]
                    
                    # ë² ìŠ¤íŠ¸ ë¦¬ë·° ì¶”ê°€
                    if 'reviews' in details:
                        best_review = max(details['reviews'], key=lambda x: x.get('rating', 0))
                        place_info_obj.best_review = best_review.get('text')
                    
                    place_details.append(place_info_obj)
                    print(f"ì¥ì†Œ ì •ë³´ ì¶”ê°€ ì™„ë£Œ: {place_name}")
                except Exception as e:
                    print(f"ì¥ì†Œ ì •ë³´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({place_info}): {str(e)}")
                    continue
            
            return place_details
            
        except Exception as e:
            raise Exception(f"URL ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    def _process_naver_blog(self, url: str) -> List[PlaceInfo]:
        """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê¸€ì„ ì²˜ë¦¬í•˜ì—¬ ìš”ì•½ì„ ìƒì„±, YouTubeì™€ ë™ì¼í•œ íë¦„ìœ¼ë¡œ ì²˜ë¦¬"""
        try:
            # ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì»¨í…ì¸  ì¶”ì¶œ (ì „ìš© ë©”ì„œë“œ ì‚¬ìš©)
            content = self.content_service._get_naver_blog_content(url)

            # í…ìŠ¤íŠ¸ ë¶„í•  ë° ìš”ì•½
            chunks = self.text_service.split_text(content)
            summary = self.text_service.summarize_text(chunks)

            # ìš”ì•½ í…ìŠ¤íŠ¸ì—ì„œ ì¥ì†Œ ì¶”ì¶œ
            place_names = self.place_service.extract_place_names(summary)
            print(f"ì¶”ì¶œëœ ì¥ì†Œ: {place_names}")

            # ì¥ì†Œ ì •ë³´ ìˆ˜ì§‘
            place_details = []
            for place_name in place_names:
                try:
                    # ì¥ì†Œëª…ê³¼ ì§€ì—­ëª… ë¶„ë¦¬
                    if " (" in place_name and ")" in place_name:
                        place_name, area = place_name.split(" (")
                        area = area.rstrip(")")
                    else:
                        place_name = place_name
                        area = "ì¼ë³¸"

                    # Google Places APIë¡œ ì¥ì†Œ ì •ë³´ ê²€ìƒ‰
                    places_result = self.gmaps.places(f"{place_name} {area}")
                    if not places_result['results']:
                        print(f"ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: {place_name} {area}")
                        continue

                    # ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ ê²°ê³¼ë¡œ ì§„í–‰
                    place = places_result['results'][0]
                    place_id = place['place_id']
                    details = self.gmaps.place(place_id, language='ko')['result']

                    # ì¥ì†Œ íƒ€ì… ë° ì¢Œí‘œ ì •ë³´ ì¶”ì¶œ
                    place_type = details.get('types', ['unknown'])[0]
                    location = details.get('geometry', {}).get('location', {})
                    geometry = PlaceGeometry(
                        latitude=location.get('lat'),
                        longitude=location.get('lng')
                    )

                    # ìƒˆë¡œìš´ PlaceInfo ê°ì²´ ìƒì„±
                    place_info_obj = PlaceInfo(
                        name=place_name,
                        source_url=url,
                        type=place_type,
                        geometry=geometry,
                        description=self._extract_place_description(summary, place_name),
                        official_description=self._get_place_description_from_openai(place_name, place_type),
                        formatted_address=details.get('formatted_address'),
                        rating=details.get('rating'),
                        phone=details.get('formatted_phone_number'),
                        website=details.get('website'),
                        price_level=details.get('price_level'),
                        opening_hours=details.get('opening_hours', {}).get('weekday_text'),
                        google_info=details
                    )

                    # ì‚¬ì§„ URL ì¶”ê°€
                    if 'photos' in details:
                        photo_ref = details['photos'][0]['photo_reference']
                        photo_url = f"https://maps.googleapis.com/maps/api/place/photo?maxwidth=400&photoreference={photo_ref}&key={os.getenv('GOOGLE_PLACES_API_KEY')}"
                        place_info_obj.photos = [PlacePhoto(url=photo_url)]

                    # ë² ìŠ¤íŠ¸ ë¦¬ë·° ì¶”ê°€
                    if 'reviews' in details:
                        best_review = max(details['reviews'], key=lambda x: x.get('rating', 0))
                        place_info_obj.best_review = best_review.get('text')

                    place_details.append(place_info_obj)
                    print(f"ì¥ì†Œ ì •ë³´ ì¶”ê°€ ì™„ë£Œ: {place_name}")
                except Exception as e:
                    print(f"ì¥ì†Œ ì •ë³´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({place_name}): {str(e)}")
                    # ì—ëŸ¬ ë°œìƒ ì‹œ ê¸°ë³¸ ì •ë³´ë¡œ PlaceInfo ì¶”ê°€
                    place_details.append(PlaceInfo(
                        name=place_name,
                        source_url=url,
                        description=self._extract_place_description(summary, place_name),
                        google_info={}
                    ))
                    continue

            return place_details
        except Exception as e:
            raise Exception(f"URL ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    def _extract_place_description(self, summary: str, place_name: str) -> str:
        """ìš”ì•½ í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ì • ì¥ì†Œì— ëŒ€í•œ ì„¤ëª…ì„ ì¶”ì¶œ"""
        try:
            lines = summary.split('\n')
            description = ""
            
            for i, line in enumerate(lines):
                if place_name in line:
                    # í˜„ì¬ ì¤„ê³¼ ë‹¤ìŒ ëª‡ ì¤„ì„ í¬í•¨í•˜ì—¬ ì„¤ëª… ì¶”ì¶œ
                    description = ' '.join(lines[i:i+3])
                    break
            
            return description.strip() or "ì¥ì†Œ ì„¤ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        except Exception as e:
            print(f"ì¥ì†Œ ì„¤ëª… ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return "ì¥ì†Œ ì„¤ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    def _format_final_result(self, content_infos: List[ContentInfo], place_details: List[PlaceInfo], processing_time: float, urls: List[str]) -> str:
        """ìµœì¢… ê²°ê³¼ ë¬¸ìì—´ì„ í¬ë§·íŒ…í•˜ëŠ” ë©”ì„œë“œ"""
        
        # 1. ê¸°ë³¸ ì •ë³´ í—¤ë”
        final_result = f"""
=== ì—¬í–‰ ì •ë³´ ìš”ì•½ ===
ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ

ë¶„ì„í•œ ì˜ìƒ:
{'='*50}"""
        
        # 2. ë¹„ë””ì˜¤ ì •ë³´
        if content_infos:
            for info in content_infos:
                final_result += f"""
ì œëª©: {info.title}
ì±„ë„: {info.author}
URL: {info.url}"""
        else:
            final_result += f"\nURL: {chr(10).join(urls)}"
        
        final_result += f"\n{'='*50}\n\n=== ì¥ì†Œë³„ ìƒì„¸ ì •ë³´ ===\n\n"

        # ì¥ì†Œ í•„í„°ë§ ì¡°ê±´ ìˆ˜ì •
        def is_valid_place(p):
            # 1. ì¼ë³¸ ì£¼ì†Œ í™•ì¸
            if not p.formatted_address or not any(keyword in p.formatted_address for keyword in ["æ—¥æœ¬", "Japan", "ì¼ë³¸"]):
                return False
            
            # 2. ì‚¬ì§„ URL ì¡´ì¬ í™•ì¸ (nullì´ ì•„ë‹˜)
            if not p.photos or len(p.photos) == 0:
                return False
            
            # 3. ìœ„ë„/ê²½ë„ í•„ìˆ˜ í™•ì¸
            if not p.geometry or p.geometry.latitude is None or p.geometry.longitude is None:
                return False
            
            return True
        
        valid_places = [p for p in place_details if is_valid_place(p)]
        
        for idx, place in enumerate(valid_places, 1):
            final_result += f"{idx}. {place.name}\n"
            final_result += "=" * 50 + "\n\n"
            final_result += f"ì£¼ì†Œ: {place.formatted_address}\n"
            if place.geometry and place.geometry.latitude is not None and place.geometry.longitude is not None:
                final_result += f"ìœ„ë„: {place.geometry.latitude}\n"
                final_result += f"ê²½ë„: {place.geometry.longitude}\n"
            if place.photos and len(place.photos) > 0:
                final_result += f"ì‚¬ì§„ URL: {place.photos[0].url}\n"
            final_result += "=" * 50 + "\n\n"

        return final_result
    
    def search_content(self, query: str) -> List[Dict]:
        """ë²¡í„° DBì—ì„œ ì½˜í…ì¸  ê²€ìƒ‰"""
        try:
            results = self.repository.query_vectordb(query)
            filtered_results = []

            for doc in results:
                if isinstance(doc, Document) and hasattr(doc, "metadata") and hasattr(doc, "page_content"):
                    filtered_results.append({
                        'content': doc.page_content,
                        'metadata': doc.metadata
                    })
                else:
                    print(f"âš ï¸ ì˜ëª»ëœ ë°ì´í„° íƒ€ì… ê°ì§€: {type(doc)} - {doc}")

            return filtered_results
        except Exception as e:
            raise Exception(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    @staticmethod
    def _get_youtube_transcript(video_id: str) -> str:
        """YouTube ì˜ìƒì˜ ìë§‰ì„ ê°€ì ¸ì˜´"""
        try:
            print(f"\n=== ìë§‰ ì¶”ì¶œ ì‹œì‘: {video_id} ===")
            transcripts = YouTubeTranscriptApi.list_transcripts(video_id)
            
            # 1. í•œêµ­ì–´ ìë§‰ ì‹œë„
            try:
                transcript = transcripts.find_transcript(['ko'])
                transcript_list = transcript.fetch()
                print("âœ… í•œêµ­ì–´ ìë§‰ ì°¾ìŒ")
                
                # ìë§‰ í…ìŠ¤íŠ¸ êµ¬ì„±
                transcript_text = []
                for entry in transcript_list:
                    timestamp = YouTubeService._format_timestamp(entry['start'])
                    text = entry['text'].strip()
                    if text:  # ë¹ˆ í…ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
                        transcript_text.append(f"[{timestamp}] {text}")
                
                result = "\n".join(transcript_text)
                print(f"ğŸ“ ì¶”ì¶œëœ í•œêµ­ì–´ ìë§‰ ê¸¸ì´: {len(result)} ì")
                print("=== ìë§‰ ì¼ë¶€ ===")
                print(result[:500])  # ì²˜ìŒ 500ìë§Œ ì¶œë ¥
                return result
                
            except (TranscriptsDisabled, NoTranscriptFound) as e:
                print(f"âš ï¸ í•œêµ­ì–´ ìë§‰ ì—†ìŒ: {str(e)}")

            # 2. ìë™ ìƒì„±ëœ í•œêµ­ì–´ ìë§‰ ì‹œë„
            try:
                transcript = transcripts.find_generated_transcript(['ko'])
                transcript_list = transcript.fetch()
                print("âœ… ìë™ ìƒì„±ëœ í•œêµ­ì–´ ìë§‰ ì°¾ìŒ")
                
                transcript_text = []
                for entry in transcript_list:
                    timestamp = YouTubeService._format_timestamp(entry['start'])
                    text = entry['text'].strip()
                    if text:
                        transcript_text.append(f"[{timestamp}] {text}")
                
                result = "\n".join(transcript_text)
                print(f"ğŸ“ ì¶”ì¶œëœ ìë™ ìƒì„± í•œêµ­ì–´ ìë§‰ ê¸¸ì´: {len(result)} ì")
                print("=== ìë§‰ ì¼ë¶€ ===")
                print(result[:500])
                return result
                
            except Exception as e:
                print(f"âš ï¸ ìë™ ìƒì„±ëœ í•œêµ­ì–´ ìë§‰ ì—†ìŒ: {str(e)}")

            # 3. ì˜ì–´ ìë§‰ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­
            try:
                transcript = transcripts.find_transcript(['en'])
                transcript_list = transcript.fetch()
                print("âœ… ì˜ì–´ ìë§‰ ì°¾ìŒ")
                
                # ì˜ì–´ ìë§‰ í…ìŠ¤íŠ¸ êµ¬ì„±
                en_texts = []
                timestamps = []
                for entry in transcript_list:
                    timestamp = YouTubeService._format_timestamp(entry['start'])
                    text = entry['text'].strip()
                    if text:
                        en_texts.append(text)
                        timestamps.append(timestamp)
                
                # OpenAIë¥¼ ì‚¬ìš©í•˜ì—¬ í•œêµ­ì–´ë¡œ ë²ˆì—­
                combined_text = " ".join(en_texts)
                response = openai.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "system", "content": "You are a professional translator specializing in Korean travel content."},
                        {"role": "user", "content": f"Translate the following English text to Korean, maintaining the travel-related context:\n\n{combined_text}"}
                    ],
                    temperature=0.3
                )
                
                translated_text = response.choices[0].message.content
                
                # ë²ˆì—­ëœ í…ìŠ¤íŠ¸ë¥¼ íƒ€ì„ìŠ¤íƒ¬í”„ì™€ ê²°í•©
                result = f"[ë²ˆì—­ëœ ìë§‰]\n"
                sentences = translated_text.split('. ')
                for i, (timestamp, sentence) in enumerate(zip(timestamps, sentences)):
                    if sentence.strip():
                        result += f"[{timestamp}] {sentence.strip()}\n"
                
                print(f"ğŸ“ ë²ˆì—­ëœ ìë§‰ ê¸¸ì´: {len(result)} ì")
                print("=== ë²ˆì—­ëœ ìë§‰ ì¼ë¶€ ===")
                print(result[:500])
                return result
                
            except Exception as e:
                print(f"âš ï¸ ì˜ì–´ ìë§‰ ë³€í™˜ ì‹¤íŒ¨: {str(e)}")

            raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ ìë§‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        except Exception as e:
            print(f"âŒ ìë§‰ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            raise ValueError(f"ë¹„ë””ì˜¤ {video_id}ì˜ ìë§‰ì„ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")

    @staticmethod
    def _format_timestamp(seconds: float) -> str:
        """ì´ˆë¥¼ ì‹œ:ë¶„:ì´ˆ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        return f"{hours:02d}:{minutes:02d}:{secs:02d}"

    def _get_video_info(self, video_id: str) -> VideoInfo:
        """YouTube ë¹„ë””ì˜¤ ì •ë³´ë¥¼ ê°€ì ¸ì˜´"""
        try:
            import requests
            
            # noembed APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë””ì˜¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            api_url = f"https://noembed.com/embed?url=https://www.youtube.com/watch?v={video_id}"
            response = requests.get(api_url)
            
            if response.status_code == 200:
                data = response.json()
                return VideoInfo(
                    url=f"https://www.youtube.com/watch?v={video_id}",
                    title=data.get('title'),
                    channel=data.get('author_name')
                )
            else:
                print(f"[get_video_info] API ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {response.status_code}")
                return VideoInfo(
                    url=f"https://www.youtube.com/watch?v={video_id}",
                    title="ì œëª©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ",
                    channel="ì±„ë„ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ"
                )
            
        except Exception as e:
            print(f"ë¹„ë””ì˜¤ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
            return VideoInfo(
                url=f"https://www.youtube.com/watch?v={video_id}",
                title="ì œëª©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ",
                channel="ì±„ë„ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ"
            )

    def _get_blog_info(self, url: str) -> Dict[str, str]:
        """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜´"""
        try:
            import requests
            from bs4 import BeautifulSoup
            
            response = requests.get(url)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë¸”ë¡œê·¸ ì œëª© ì¶”ì¶œ ì‹œë„
            title = None
            title_tag = soup.find('meta', property='og:title')
            if title_tag:
                title = title_tag.get('content')
            
            if not title:
                title_tag = soup.find('title')
                if title_tag:
                    title = title_tag.text
            
            # ì‘ì„±ì ì •ë³´ ì¶”ì¶œ ì‹œë„
            author = None
            author_tag = soup.find('meta', property='og:article:author')
            if author_tag:
                author = author_tag.get('content')
            
            if not author:
                # ë¸”ë¡œê·¸ URLì—ì„œ ì‘ì„±ì ID ì¶”ì¶œ
                try:
                    blog_id = url.split('blog.naver.com/')[1].split('/')[0]
                    author = f"ë„¤ì´ë²„ ë¸”ë¡œê·¸ | {blog_id}"
                except:
                    author = "ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì‘ì„±ì"
            
            return {
                'title': title or "ì œëª©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ",
                'author': author or "ì‘ì„±ì ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ"
            }
            
        except Exception as e:
            print(f"ë¸”ë¡œê·¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
            return {
                'title': "ì œëª©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ",
                'author': "ì‘ì„±ì ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ"
            }

    def generate_final_summary(self, content_infos: List[ContentInfo], processing_time: float, place_details: List[PlaceInfo]) -> Dict[str, str]:
        """ìµœì¢… ìš”ì•½ì„ ìƒì„±"""
        summaries = {}
        
        for content in content_infos:
            summary = f"=== ì—¬í–‰ ì •ë³´ ìš”ì•½ ===\n"
            summary += f"ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ\n\n"
            
            # ë¶„ì„í•œ ì½˜í…ì¸  ì •ë³´
            summary += "ë¶„ì„í•œ ì½˜í…ì¸ :\n"
            summary += "=" * 50 + "\n"
            for idx, info in enumerate(content_infos, 1):
                summary += f"{idx}. {info.platform.value.upper()}\n"
                summary += f"ì œëª©: {info.title}\n"
                summary += f"ì‘ì„±ì: {info.author}\n"
                summary += f"URL: {info.url}\n\n"
            
            summary += "=" * 50 + "\n\n"
            
            # ì¥ì†Œë³„ ìƒì„¸ ì •ë³´
            summary += "=== ì¥ì†Œë³„ ìƒì„¸ ì •ë³´ ===\n\n"
            
            # Filtering valid places according to the criteria
            def is_valid_place(p):
                # 1. ì¼ë³¸ ì£¼ì†Œ í™•ì¸
                if not p.formatted_address or not any(keyword in p.formatted_address for keyword in ["æ—¥æœ¬", "Japan", "ì¼ë³¸"]):
                    return False
                
                # 2. ì‚¬ì§„ URL ì¡´ì¬ í™•ì¸ (nullì´ ì•„ë‹˜)
                if not p.photos or len(p.photos) == 0:
                    return False
                
                # 3. ìœ„ë„/ê²½ë„ í•„ìˆ˜ í™•ì¸
                if not p.geometry or p.geometry.latitude is None or p.geometry.longitude is None:
                    return False
                
                return True
            
            valid_places = [p for p in place_details if is_valid_place(p)]
            
            for idx, place in enumerate(valid_places, 1):
                summary += f"{idx}. {place.name}\n"
                summary += "=" * 50 + "\n\n"
                summary += f"ì£¼ì†Œ: {place.formatted_address}\n"
                if place.geometry and place.geometry.latitude is not None and place.geometry.longitude is not None:
                    summary += f"ìœ„ë„: {place.geometry.latitude}\n"
                    summary += f"ê²½ë„: {place.geometry.longitude}\n"
                if place.photos and len(place.photos) > 0:
                    summary += f"ì‚¬ì§„ URL: {place.photos[0].url}\n"
                summary += "=" * 50 + "\n\n"
            
            # ìœ íš¨í•œ ì¥ì†Œê°€ ì—†ëŠ” ê²½ìš° ë©”ì‹œì§€ ì¶”ê°€
            if not valid_places:
                summary += "â€» ìœ íš¨í•œ ì¥ì†Œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. (ì‚¬ì§„, ì¢Œí‘œ, ì¼ë³¸ ì£¼ì†Œ ì¤‘ í•˜ë‚˜ ì´ìƒ ëˆ„ë½)\n"
            
            summaries[content.url] = summary
        
        return summaries

    def _get_place_description_from_openai(self, place_name: str, place_type: str) -> str:
        """OpenAIë¥¼ ì‚¬ìš©í•˜ì—¬ ì¥ì†Œì— ëŒ€í•œ ì¼ë°˜ì ì¸ ì„¤ëª… ìƒì„±"""
        try:
            prompt = f"""ë‹¤ìŒ ì¥ì†Œì— ëŒ€í•œ ì •í™•í•˜ê³  ê°„ê²°í•œ ì„¤ëª…ì„ ì œê³µí•˜ì„¸ìš”.  
ì„¤ëª…ì€ 10ìë¡œ ì œí•œë˜ë©°, í•µì‹¬ ì •ë³´ë§Œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
ì¥ì†Œ: {place_name}
íƒ€ì…: {place_type}
ë°˜ë“œì‹œ ì§§ê³  ëª…í™•í•œ í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”."""

            response = openai.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì¼ë³¸ ì „ë¬¸ ì—¬í–‰ ê°€ì´ë“œì…ë‹ˆë‹¤. ì¥ì†Œì— ëŒ€í•œ ê°ê´€ì ì´ê³  ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=30
            )
            
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"ì¥ì†Œ ì„¤ëª… ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return None

    def process_place_info(self, place_name: str, timestamp: str, description: str) -> PlaceInfo:
        """PlaceServiceì˜ process_place_infoë¥¼ í˜¸ì¶œ"""
        return self.place_service.process_place_info(place_name, timestamp, description)

class YouTubeSubtitleService:
    """YouTube ìë§‰ ë° ë¹„ë””ì˜¤ ì •ë³´ ì²˜ë¦¬ ì„œë¹„ìŠ¤"""
    
    @staticmethod
    def get_video_info(video_url: str) -> Tuple[str, str]:
        try:
            video_id = video_url.split("v=")[-1].split("&")[0]
            api_url = f"https://noembed.com/embed?url=https://www.youtube.com/watch?v={video_id}"
            response = requests.get(api_url)
            if response.status_code == 200:
                data = response.json()
                title = data.get('title')
                author_name = data.get('author_name')
                print(f"[get_video_info] ì œëª©: {title}, ì±„ë„: {author_name}")
                return title, author_name
            print(f"[get_video_info] API ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {response.status_code}")
            return None, None
        except Exception as e:
            print(f"ì˜ìƒ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
            return None, None

    @staticmethod
    def process_link(url: str) -> str:
        link_type = YouTubeSubtitleService._detect_link_type(url)
        print(f"[process_link] ë§í¬ ìœ í˜• ê°ì§€: {link_type}")
        
        if link_type == "youtube":
            text = YouTubeSubtitleService._get_youtube_transcript(url)
        elif link_type == "text_file":
            text = YouTubeSubtitleService._get_text_from_file(url)
        else:
            text = YouTubeSubtitleService._get_text_from_webpage(url)
        
        print(f"[process_link] ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}")
        return text

    @staticmethod
    def _detect_link_type(url: str) -> str:
        if "youtube.com" in url or "youtu.be" in url:
            return "youtube"
        elif url.endswith(".txt"):
            return "text_file"
        elif url.startswith("http"):
            return "webpage"
        else:
            return "unknown"

    @staticmethod
    def _format_timestamp(seconds: float) -> str:
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        return f"{hours:02d}:{minutes:02d}:{secs:02d}"

    @staticmethod
    def _get_youtube_transcript(video_url: str) -> str:
        video_id = video_url.split("v=")[-1].split("&")[0]
        print(f"[get_youtube_transcript] ë¹„ë””ì˜¤ ID: {video_id}")
        
        try:
            transcripts = YouTubeTranscriptApi.list_transcripts(video_id)
            
            # 1. ë¨¼ì € í•œêµ­ì–´ ìë§‰ ì‹œë„
            try:
                transcript = transcripts.find_transcript(['ko'])
                transcript_text = "\n".join([f"[{YouTubeSubtitleService._format_timestamp(entry['start'])}] {entry['text']}" for entry in transcript.fetch()])
                print(f"[get_youtube_transcript] í•œêµ­ì–´ ìë§‰ ì¶”ì¶œ ì™„ë£Œ. ê¸¸ì´: {len(transcript_text)}")
                return transcript_text
            except (TranscriptsDisabled, NoTranscriptFound):
                print("[get_youtube_transcript] í•œêµ­ì–´ ìë§‰ ì—†ìŒ.")

            # 2. ì˜ì–´ ìë§‰ ì‹œë„
            try:
                transcript = transcripts.find_transcript(['en'])
                transcript_text = "\n".join([f"[{YouTubeSubtitleService._format_timestamp(entry['start'])}] {entry['text']}" for entry in transcript.fetch()])
                print(f"[get_youtube_transcript] ì˜ì–´ ìë§‰ ì¶”ì¶œ ì™„ë£Œ. ê¸¸ì´: {len(transcript_text)}")
                return transcript_text
            except (TranscriptsDisabled, NoTranscriptFound):
                print("[get_youtube_transcript] ì˜ì–´ ìë§‰ ì—†ìŒ.")

            # 3. ì‚¬ìš© ê°€ëŠ¥í•œ ì²« ë²ˆì§¸ ìë§‰ ì‹œë„
            try:
                transcript = transcripts.find_generated_transcript()
                transcript_text = "\n".join([f"[{YouTubeSubtitleService._format_timestamp(entry['start'])}] {entry['text']}" for entry in transcript.fetch()])
                print(f"[get_youtube_transcript] ìƒì„±ëœ ìë§‰ ì¶”ì¶œ ì™„ë£Œ. ê¸¸ì´: {len(transcript_text)}")
                return transcript_text
            except Exception as e:
                print(f"[get_youtube_transcript] ìƒì„±ëœ ìë§‰ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

            raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ ìë§‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        except Exception as e:
            raise ValueError(f"ë¹„ë””ì˜¤ {video_id}ì˜ ìë§‰ì„ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")

    @staticmethod
    def _get_text_from_file(url: str) -> str:
        try:
            response = requests.get(url)
            response.raise_for_status()
            text = response.text.strip()
            print(f"[get_text_from_file] í…ìŠ¤íŠ¸ íŒŒì¼ ì¶”ì¶œ ì™„ë£Œ. ê¸¸ì´: {len(text)}")
            return text
        except Exception as e:
            raise ValueError(f"í…ìŠ¤íŠ¸ íŒŒì¼ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ”ë° ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    @staticmethod
    def _get_text_from_webpage(url: str) -> str:
        try:
            response = requests.get(url)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, "html.parser")
            text = soup.get_text(separator="\n").strip()
            text = text[:10000]  # ê¸¸ì´ ì œí•œ 10000ì
            print(f"[get_text_from_webpage] ì›¹í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ. ê¸¸ì´: {len(text)}")
            return text
        except Exception as e:
            raise ValueError(f"ì›¹í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ”ë° ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

class TextProcessingService:
    """í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì„œë¹„ìŠ¤"""
    
    def split_text(self, text: str, max_chunk_size: int = 2048) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        try:
            words = text.split()
            total_words = len(words)
            num_chunks = ceil(total_words / (max_chunk_size // 5))
            chunks = []
            
            for i in range(num_chunks):
                start = i * (max_chunk_size // 5)
                end = start + (max_chunk_size // 5)
                chunk = ' '.join(words[start:end])
                chunks.append(chunk)
            
            print(f"[split_text] ì´ ë‹¨ì–´ ìˆ˜: {total_words}, ì²­í¬ ìˆ˜: {num_chunks}")
            return chunks
            
        except Exception as e:
            print(f"[split_text] ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise ValueError(f"í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    def _generate_prompt(self, text: str) -> str:
        """GPT í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        return f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ì—¬í–‰ ì •ë³´ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”. 
íŠ¹íˆ ë‹¤ìŒ ì‚¬í•­ì— ì¤‘ì ì„ ë‘ì–´ ìš”ì•½í•´ì£¼ì„¸ìš”:

1. ë°©ë¬¸í•œ ì¥ì†Œë“¤ (ìœ„ì¹˜ ì •ë³´ í¬í•¨)
2. ê° ì¥ì†Œì˜ íŠ¹ì§•ê³¼ ì„¤ëª…
3. ì¶”ì²œ ì‚¬í•­ì´ë‚˜ ì£¼ì˜ ì‚¬í•­
4. ì‹œê°„ëŒ€ë³„ ë°©ë¬¸ ì •ë³´ (ìˆëŠ” ê²½ìš°)

í…ìŠ¤íŠ¸:
{text}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

ë°©ë¬¸í•œ ì¥ì†Œ: [ì¥ì†Œëª…] ([ì§€ì—­ëª…])
- ì„¤ëª…: [ì¥ì†Œì— ëŒ€í•œ ì„¤ëª…]
- ì¶”ì²œ ì‚¬í•­: [ìˆëŠ” ê²½ìš°]
- ì£¼ì˜ ì‚¬í•­: [ìˆëŠ” ê²½ìš°]
- ë°©ë¬¸ ì‹œê°„: [ì–¸ê¸‰ëœ ê²½ìš°]
"""

    def summarize_text(self, transcript_chunks: List[str], model: str = "gpt-4o-mini") -> str:
        """í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì„ ìš”ì•½"""
        try:
            summaries = []
            for idx, chunk in enumerate(transcript_chunks):
                print(f"[summarize_text] ì²­í¬ {idx+1}/{len(transcript_chunks)} ì²˜ë¦¬ ì¤‘...")
                
                prompt = self._generate_prompt(chunk)
                response = openai.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ì—¬í–‰ ì „ë¬¸ê°€ë¡œì„œ ì—¬í–‰ ì»¨í…ì¸ ë¥¼ ë¶„ì„í•˜ê³  ìœ ìš©í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” AIì…ë‹ˆë‹¤."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.3,
                    max_tokens=1500
                )
                
                summary = response.choices[0].message.content
                summaries.append(summary)
                print(f"[summarize_text] ì²­í¬ {idx+1} ìš”ì•½ ì™„ë£Œ")
            
            return "\n\n".join(summaries)
            
        except Exception as e:
            print(f"[summarize_text] ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise ValueError(f"í…ìŠ¤íŠ¸ ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

class PlaceService:
    """ì¥ì†Œ ì •ë³´ ì²˜ë¦¬ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.video_url = None

    def set_video_url(self, url: str):
        self.video_url = url

    def extract_place_names(self, summary: str) -> List[str]:
        """ìš”ì•½ í…ìŠ¤íŠ¸ì—ì„œ ì¥ì†Œ ì´ë¦„ì„ ì¶”ì¶œ"""
        place_names = set()  # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ set ì‚¬ìš©
        
        # ëª¨ë“  ì²­í¬ì˜ ìš”ì•½ì—ì„œ ì¥ì†Œ ì¶”ì¶œ
        chunks = summary.split("ë°©ë¬¸í•œ ì¥ì†Œ:")
        for chunk in chunks[1:]:  # ì²« ë²ˆì§¸ëŠ” ê±´ë„ˆë›°ê¸°
            try:
                place_name = chunk.split("(")[0].strip()
                if place_name:
                    place_names.add(place_name)
                    print(f"ì¥ì†Œ ì¶”ì¶œ: {place_name}")
            except Exception as e:
                print(f"ì¥ì†Œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
                continue
        
        result = list(place_names)
        print(f"ì´ ì¶”ì¶œëœ ì¥ì†Œ ëª©ë¡: {result}")
        return result

    @staticmethod
    def search_place_details(place_name: str, area: str = None) -> Dict[str, Any]:
        """Google Places APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì¥ì†Œ ì •ë³´ë¥¼ ê²€ìƒ‰"""
        try:
            gmaps = googlemaps.Client(key=os.getenv("GOOGLE_PLACES_API_KEY"))
            
            # ì§€ì—­ëª…ì´ ìˆìœ¼ë©´ ì¥ì†Œëª…ê³¼ í•¨ê»˜ ê²€ìƒ‰, ì—†ìœ¼ë©´ 'ì¼ë³¸'ì„ ì¶”ê°€
            search_query = f"{place_name} {area if area else 'ì¼ë³¸'}"
            print(f"[search_place_details] ê²€ìƒ‰ì–´: {search_query}")
            
            # ì¥ì†Œ ê²€ìƒ‰
            places_result = gmaps.places(search_query)
            
            if not places_result['results']:
                print(f"[search_place_details] ì¥ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {search_query}")
                return None
                
            place = places_result['results'][0]
            place_id = place['place_id']
            
            # ìƒì„¸ ì •ë³´ ê²€ìƒ‰
            details_result = gmaps.place(place_id, language='ko')
            if not details_result.get('result'):
                return None
                
            details = details_result['result']
            
            # ë¦¬ë·° ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            reviews = details.get('reviews', [])
            best_review = reviews[0]['text'] if reviews else None
            
            # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ìƒì„±
            return {
                'name': details.get('name', ''),
                'formatted_address': details.get('formatted_address', ''),
                'rating': details.get('rating'),
                'formatted_phone_number': details.get('formatted_phone_number', ''),
                'website': details.get('website', ''),
                'price_level': details.get('price_level'),
                'opening_hours': details.get('opening_hours', {}).get('weekday_text', []),
                'photos': details.get('photos', []),
                'best_review': bestreview
            }
            
        except Exception as e:
            print(f"[search_place_details] ì¥ì†Œ ì •ë³´ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({search_query}): {str(e)}")
            return None

    @staticmethod
    def get_place_photo_google(place_name: str) -> str:
        """Google Places APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì¥ì†Œ ì‚¬ì§„ URLì„ ê°€ì ¸ì˜´"""
        try:
            gmaps = googlemaps.Client(key=os.getenv("GOOGLE_PLACES_API_KEY"))
            places_result = gmaps.places(place_name)
            
            if not places_result['results']:
                print(f"[get_place_photo_google] ì‚¬ì§„ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {place_name}")
                return None
                
            place = places_result['results'][0]
            if not place.get('photos'):
                return None
                
            photo_reference = place['photos'][0]['photo_reference']
            photo_url = f"https://maps.googleapis.com/maps/api/place/photo?maxwidth=400&photoreference={photo_reference}&key={os.getenv('GOOGLE_PLACES_API_KEY')}"
            
            print(f"[get_place_photo_google] ì‚¬ì§„ URL ìƒì„± ì™„ë£Œ: {photo_url}")
            return photo_url
            
        except Exception as e:
            print(f"[get_place_photo_google] ì‚¬ì§„ URL ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return None

    def process_place_info(self, place_name: str, timestamp: str, description: str) -> PlaceInfo:
        """ì¥ì†Œ ì •ë³´ë¥¼ ì²˜ë¦¬í•˜ê³  PlaceInfo ê°ì²´ë¥¼ ë°˜í™˜"""
        try:
            # Google Places APIë¡œ ì¥ì†Œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            gmaps = googlemaps.Client(key=os.getenv("GOOGLE_PLACES_API_KEY"))
            places_result = gmaps.places(place_name)
            
            if not places_result['results']:
                return None
            
            google_place_info = places_result['results'][0]
            
            # ì‚¬ì§„ URL ê°€ì ¸ì˜¤ê¸°
            photo_url = self.get_place_photo_google(place_name)
            
            # ì¥ì†Œ íƒ€ì… í™•ì¸
            place_type = google_place_info.get('types', ['unknown'])[0]
            
            # OpenAIë¡œ ê³µì‹ ì„¤ëª… ìƒì„±
            official_description = self._get_place_description_from_openai(place_name, place_type)
            
            # ì˜ì—…ì‹œê°„ í¬ë§·íŒ…
            opening_hours = None
            if google_place_info.get('opening_hours'):
                opening_hours = google_place_info['opening_hours'].get('weekday_text')

            # PlaceInfo ê°ì²´ ìƒì„±
            place_info = PlaceInfo(
                name=place_name,
                source_url=self.video_url,
                timestamp=timestamp,
                description=description,
                official_description=official_description,
                formatted_address=google_place_info.get('formatted_address'),
                coordinates={
                    'lat': google_place_info['geometry']['location']['lat'],
                    'lng': google_place_info['geometry']['location']['lng']
                } if 'geometry' in google_place_info else None,
                rating=google_place_info.get('rating'),
                phone=google_place_info.get('formatted_phone_number'),
                website=google_place_info.get('website'),
                price_level=google_place_info.get('price_level'),
                opening_hours=opening_hours,
                photos=[PlacePhoto(url=photo_url)] if photo_url else None,
                best_review=google_place_info.get('reviews', [{}])[0].get('text') if google_place_info.get('reviews') else None,
                google_info=google_place_info,
                types=google_place_info.get('types')
            )
            
            return place_info
            
        except Exception as e:
            print(f"ì¥ì†Œ ì •ë³´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return None
