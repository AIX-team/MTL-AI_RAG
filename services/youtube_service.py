import time
import os
import requests
import datetime
import tiktoken
from cachetools import TTLCache
from math import ceil
from langdetect import detect
from dotenv import load_dotenv
from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound
from bs4 import BeautifulSoup
import openai
from googleapiclient.discovery import build
import googlemaps
from typing import List, Dict, Tuple, Any
from models.youtube_schemas import YouTubeResponse, VideoInfo, PlaceInfo, PlacePhoto, ContentType, ContentInfo, PlaceGeometry
from repository.youtube_repository import YouTubeRepository
from langchain.schema import Document
from urllib.parse import urlparse, parse_qs
from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from fastapi import APIRouter, HTTPException, status



from ai_api.youtube_subtitle import (
    get_video_info, process_link, split_text, summarize_text,
    extract_place_names, search_place_details, get_place_photo_google
)

# í™˜ê²½ ë³€ìˆ˜ ë° ìƒìˆ˜ ì„¤ì •
load_dotenv(dotenv_path=".env")

# API í‚¤ ì„¤ì •
openai.api_key = os.getenv("OPENAI_API_KEY")
GEOCODING_API_KEY = os.getenv("GOOGLE_GEOCODING_API_KEY")
GOOGLE_PLACES_API_KEY = os.getenv("GOOGLE_PLACES_API_KEY")

# ìƒìˆ˜ ì„¤ì •
MAX_URLS = 5
CHUNK_SIZE = 2048
MODEL = "gpt-4o-mini"
FINAL_SUMMARY_MAX_TOKENS = 1500

class ContentService:
    """ì»¨í…ì¸  ì²˜ë¦¬ ì„œë¹„ìŠ¤"""
    
    @staticmethod
    def get_content_info(url: str) -> Tuple[str, str, ContentType]:
        """URLì—ì„œ ì»¨í…ì¸  ì •ë³´ ì¶”ì¶œ"""
        content_type = ContentService._detect_content_type(url)
        
        if content_type == ContentType.YOUTUBE:
            title, author = YouTubeSubtitleService.get_video_info(url)
        elif content_type == ContentType.NAVER_BLOG:
            title, author = ContentService._get_naver_blog_info(url)
        elif content_type == ContentType.TISTORY:
            title, author = ContentService._get_tistory_blog_info(url)
        else:
            title, author = ContentService._get_webpage_info(url)
            
        return title, author, content_type

    @staticmethod
    def _detect_content_type(url: str) -> ContentType:
        """URL ìœ í˜• ê°ì§€"""
        domain = urlparse(url).netloc.lower()
        
        if "youtube.com" in domain or "youtu.be" in domain:
            return ContentType.YOUTUBE
        elif "blog.naver.com" in domain:
            return ContentType.NAVER_BLOG
        elif ".tistory.com" in domain:
            return ContentType.TISTORY
        elif url.endswith(".txt"):
            return ContentType.TEXT_FILE
        elif url.startswith("http"):
            return ContentType.WEBPAGE
        return ContentType.UNKNOWN

    @staticmethod
    def _get_naver_blog_info(url: str) -> Tuple[str, str]:
        """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì •ë³´ ì¶”ì¶œ"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(url, headers=headers)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # iframe ë‚´ë¶€ì˜ ì‹¤ì œ ì»¨í…ì¸  URL ì°¾ê¸°
            if 'blog.naver.com' in url:
                iframe = soup.find('iframe', id='mainFrame')
                if iframe and iframe.get('src'):
                    real_url = f"https://blog.naver.com{iframe['src']}"
                    response = requests.get(real_url, headers=headers)
                    soup = BeautifulSoup(response.text, 'html.parser')
            
            title = soup.find('meta', property='og:title')
            title = title['content'] if title else "ì œëª© ì—†ìŒ"
            
            author = soup.find('meta', property='og:article:author')
            author = author['content'] if author else "ìž‘ì„±ìž ì—†ìŒ"
            
            return title, author
        except Exception as e:
            print(f"ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None, None

    @staticmethod
    def _get_tistory_blog_info(url: str) -> Tuple[str, str]:
        """í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ ì •ë³´ ì¶”ì¶œ"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(url, headers=headers)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            title = soup.find('meta', property='og:title')
            title = title['content'] if title else "ì œëª© ì—†ìŒ"
            
            author = soup.find('meta', property='og:article:author')
            author = author['content'] if author else "ìž‘ì„±ìž ì—†ìŒ"
            
            return title, author
        except Exception as e:
            print(f"í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None, None

    @staticmethod
    def process_content(url: str) -> str:
        """URLì—ì„œ ì»¨í…ì¸  ì¶”ì¶œ"""
        content_type = ContentService._detect_content_type(url)
        
        if content_type == ContentType.YOUTUBE:
            return YouTubeSubtitleService.process_link(url)
        elif content_type == ContentType.NAVER_BLOG:
            return ContentService._get_naver_blog_content(url)
        elif content_type == ContentType.TISTORY:
            return ContentService._get_tistory_blog_content(url)
        elif content_type == ContentType.TEXT_FILE:
            return YouTubeSubtitleService._get_text_from_file(url)
        else:
            return YouTubeSubtitleService._get_text_from_webpage(url)

    @staticmethod
    def _get_naver_blog_content(url: str) -> str:
        """ë„¤ì´ë²„ ë¸”ë¡œê·¸ì—ì„œ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ (ë¶ˆí•„ìš”í•œ ê°œí–‰ ë° ê³µë°±, ê´‘ê³  ì œê±° í¬í•¨)"""
        cache = TTLCache(maxsize=10, ttl=300)

        if url in cache:
            return cache[url]

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }

        def clean_text(text: str) -> str:
            import re
            text = re.sub(r'\s+', ' ', text)  # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ ë³€í™˜
            text = re.sub(r'[^\S\r\n]+', ' ', text)  # ìœ ë‹ˆì½”ë“œ ê³µë°± ì œê±°
            text = re.sub(r'Â©.*?(?= )', '', text)   # Â© ë“± ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ ì œê±°
            text = re.sub(r'\[ë°”ë¡œê°€ê¸°\]', '', text)
            return text.strip()

        try:
            # ì²« ë²ˆì§¸ ìš”ì²­ìœ¼ë¡œ iframe URL ê°€ì ¸ì˜¤ê¸°
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            # iframe ì°¾ê¸° (ìƒˆ ë²„ì „ ë¸”ë¡œê·¸)
            iframe = soup.find('iframe', id='mainFrame')
            if iframe and iframe.get('src'):
                real_url = f"https://blog.naver.com{iframe['src']}"
                response = requests.get(real_url, headers=headers)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')

            # ìƒˆë¡œìš´ ë²„ì „ ë¸”ë¡œê·¸ ì˜ì—­
            content = soup.find('div', {'class': 'se-main-container'})
            if not content:
                # êµ¬ë²„ì „ ë¸”ë¡œê·¸ ì˜ì—­
                content = soup.find('div', {'class': 'post-view'})
            if not content:
                raise HTTPException(status_code=404, detail="ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
            for tag in content.find_all(['script', 'style']):
                tag.decompose()

            text = clean_text(content.get_text(separator='\n'))
            cache[url] = text
            return text

        except requests.RequestException as e:
            raise HTTPException(status_code=500, detail=f"ë¸”ë¡œê·¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    @staticmethod
    def _get_tistory_blog_content(url: str) -> str:
        """í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ ì»¨í…ì¸  ì¶”ì¶œ"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = requests.get(url, headers=headers)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë³¸ë¬¸ ì»¨í…ì¸  ì°¾ê¸°
            content = soup.find('div', {'class': 'entry-content'})
            if not content:
                content = soup.find('div', {'class': 'article'})
            
            if content:
                # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                for element in content.find_all(['script', 'style']):
                    element.decompose()
                
                text = content.get_text(separator='\n').strip()
                return text
            
            return "ì»¨í…ì¸ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        except Exception as e:
            print(f"í‹°ìŠ¤í† ë¦¬ ë¸”ë¡œê·¸ ì»¨í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return "ì»¨í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨"

class YouTubeService:
    """ë©”ì¸ YouTube ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        """YouTubeService ì´ˆê¸°í™”"""
        from dotenv import load_dotenv
        load_dotenv()  # .env íŒŒì¼ ë¡œë“œ
        
        self.repository = YouTubeRepository()
        self.content_service = ContentService()
        self.text_service = TextProcessingService()
        self.place_service = PlaceService()
        
        # Google Maps API í‚¤ í™•ì¸ ë° ì„¤ì •
        google_maps_api_key = os.getenv('GOOGLE_PLACES_API_KEY')  # GOOGLE_MAPS_API_KEY ëŒ€ì‹  GOOGLE_PLACES_API_KEY ì‚¬ìš©
        if not google_maps_api_key:
            raise ValueError("GOOGLE_PLACES_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            self.gmaps = googlemaps.Client(key=google_maps_api_key)
        except Exception as e:
            raise ValueError(f"Google Maps í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")

    async def process_urls(self, urls: List[str]) -> Dict:
        """URL ëª©ë¡ì„ ì²˜ë¦¬í•˜ì—¬ ê°ê°ì˜ ìš”ì•½ì„ ìƒì„±"""
        try:
            content_infos = []
            place_details = []
            start_time = time.time()

            for url in urls:
                parsed_url = urlparse(url)
                if 'youtube.com' in parsed_url.netloc:
                    # YouTube ì˜ìƒ ì²˜ë¦¬
                    video_id = parse_qs(parsed_url.query).get('v', [None])[0]
                    if video_id:
                        video_info = self._get_video_info(video_id)
                        content_info = ContentInfo(
                            url=url,
                            title=video_info.title,
                            author=video_info.channel,
                            platform=ContentType.YOUTUBE
                        )
                        content_infos.append(content_info)
                        
                        video_places = self._process_youtube_video(video_id, url)
                        place_details.extend(video_places)
                        print(f"YouTube ì˜ìƒ '{video_info.title}'ì—ì„œ ì¶”ì¶œëœ ìž¥ì†Œ: {len(video_places)}ê°œ")
                
                elif 'blog.naver.com' in parsed_url.netloc:
                    # ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì œëª© ë° ìž‘ì„±ìž ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                    title, author = self.content_service._get_naver_blog_info(url)

                    # ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
                    content = self.content_service.process_content(url)  # ì—¬ê¸°ì„œ _get_naver_blog_content í•¨ìˆ˜ê°€ í˜¸ì¶œë¨
                    # ë³¸ë¬¸ì„ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸°
                    chunks = self.text_service.split_text(content)
                    # ì²­í¬ë“¤ì„ ìš”ì•½í•´ì„œ ìµœì¢… ìš”ì•½ ìƒì„±
                    summary = self.text_service.summarize_text(chunks)

                    # ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì½˜í…ì¸  ì •ë³´ ì €ìž¥
                    content_info = ContentInfo(
                        url=url,
                        title=title,
                        author=author,
                        platform=ContentType.NAVER_BLOG
                    )
                    content_infos.append(content_info)

                    # (ì›í•˜ëŠ” ê²½ìš°) ìš”ì•½ëœ ê²°ê³¼ë¥¼ ì¶”ê°€ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ë„ ìžˆìŠµë‹ˆë‹¤.
                    # ì˜ˆë¥¼ ë“¤ì–´, summaryë¥¼ ë¡œê·¸ë¡œ ë‚¨ê¸°ê±°ë‚˜ ìµœì¢… ê²°ê³¼ì— í¬í•¨ì‹œí‚¤ê¸°

                    # ë„¤ì´ë²„ ë¸”ë¡œê·¸ì—ì„œ ìž¥ì†Œ ì •ë³´ ì¶”ì¶œ
                    blog_places = self._process_naver_blog(url)
                    place_details.extend(blog_places)

                    print(f"ë„¤ì´ë²„ ë¸”ë¡œê·¸ '{title}'ì—ì„œ ì¶”ì¶œëœ ìž¥ì†Œ: {len(blog_places)}ê°œ")


            processing_time = time.time() - start_time

            # URLë³„ë¡œ ìž¥ì†Œ ì •ë³´ ê·¸ë£¹í™”
            url_places = {}
            for place in place_details:
                if place.source_url not in url_places:
                    url_places[place.source_url] = []
                url_places[place.source_url].append(place)

            # ìµœì¢… ìš”ì•½ ìƒì„±
            summaries = {}
            for content in content_infos:
                places = url_places.get(content.url, [])
                summary = self._format_final_result(
                    content_infos=[content],
                    place_details=places,
                    processing_time=processing_time,
                    urls=[content.url]
                )
                summaries[content.url] = summary
                print(f"'{content.title}' ìš”ì•½ ìƒì„± ì™„ë£Œ (ìž¥ì†Œ {len(places)}ê°œ í¬í•¨)")

            # ë²¡í„° DBì™€ íŒŒì¼ì— ì €ìž¥
            try:
                # ë²¡í„° DBì— ì €ìž¥
                await self.repository.save_to_vectordb(summaries, content_infos, place_details)
                print("âœ… ë²¡í„° DB ì €ìž¥ ì™„ë£Œ")
                
                # íŒŒì¼ë¡œ ì €ìž¥
                saved_paths = await self.repository.save_final_summary(summaries, content_infos)
                print(f"âœ… íŒŒì¼ ì €ìž¥ ì™„ë£Œ: {len(saved_paths)}ê°œ íŒŒì¼")
                
                # URLë³„ ì €ìž¥ ê²°ê³¼ ë¡œê·¸
                for content in content_infos:
                    places_count = len(url_places.get(content.url, []))
                    print(f"URL: {content.url}")
                    print(f"- ì œëª©: {content.title}")
                    print(f"- í”Œëž«í¼: {content.platform.value}")
                    print(f"- ì¶”ì¶œëœ ìž¥ì†Œ ìˆ˜: {places_count}")
                    print("-" * 50)
                
            except Exception as e:
                print(f"ì €ìž¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

            return {
                "summary": summaries,
                "content_infos": [info.dict() for info in content_infos],
                "processing_time_seconds": processing_time,
                "place_details": [place.dict() for place in place_details]
            }

        except Exception as e:
            raise ValueError(f"URL ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    def _process_youtube_video(self, video_id: str, source_url: str) -> List[PlaceInfo]:
        """YouTube ì˜ìƒì„ ì²˜ë¦¬í•˜ì—¬ ìž¥ì†Œ ì •ë³´ë¥¼ ìˆ˜ì§‘"""
        try:
            # YouTube ìžë§‰ ê°€ì ¸ì˜¤ê¸°
            transcript_text = self._get_youtube_transcript(video_id)
            
            # í…ìŠ¤íŠ¸ ë¶„í•  ë° ìš”ì•½
            chunks = self.text_service.split_text(transcript_text)
            summary = self.text_service.summarize_text(chunks)
            
            # ìž¥ì†Œ ì¶”ì¶œ ë° ì •ë³´ ìˆ˜ì§‘
            place_names = self.place_service.extract_place_names(summary)
            print(f"ì¶”ì¶œëœ ìž¥ì†Œ: {place_names}")
            
            # ìž¥ì†Œ ì •ë³´ ìˆ˜ì§‘
            place_details = []
            for place_info in place_names:
                try:
                    # ìž¥ì†Œëª…ê³¼ ì§€ì—­ëª… ë¶„ë¦¬
                    if " (" in place_info and ")" in place_info:
                        place_name, area = place_info.split(" (")
                        area = area.rstrip(")")
                    else:
                        place_name = place_info
                        area = "ì¼ë³¸"
                    
                    # Google Places APIë¡œ ìž¥ì†Œ ì •ë³´ ê²€ìƒ‰
                    places_result = self.gmaps.places(f"{place_name} {area}")
                    if places_result['results']:
                        place = places_result['results'][0]
                        place_id = place['place_id']
                        details = self.gmaps.place(place_id, language='ko')['result']
                        
                        # ìž¥ì†Œ íƒ€ìž…ê³¼ ì¢Œí‘œ ì •ë³´ ì¶”ì¶œ
                        place_type = details.get('types', ['unknown'])[0]
                        location = details.get('geometry', {}).get('location', {})
                        geometry = PlaceGeometry(
                            latitude=location.get('lat'),
                            longitude=location.get('lng')
                        )
                        
                        place_info = PlaceInfo(
                            name=place_name,
                            source_url=source_url,
                            type=place_type,  # ìž¥ì†Œ íƒ€ìž… ì„¤ì •
                            geometry=geometry,  # geometry ì •ë³´ ì„¤ì •
                            description=self._extract_place_description(summary, place_name),
                            official_description=self._get_place_description_from_openai(place_name, place_type),  # official_description ì„¤ì •
                            formatted_address=details.get('formatted_address'),
                            rating=details.get('rating'),
                            phone=details.get('formatted_phone_number'),
                            website=details.get('website'),
                            price_level=details.get('price_level'),
                            opening_hours=details.get('opening_hours', {}).get('weekday_text'),
                            google_info=details
                        )
                        
                        # ì‚¬ì§„ URL ì¶”ê°€
                        if 'photos' in details:
                            photo_ref = details['photos'][0]['photo_reference']
                            photo_url = f"https://maps.googleapis.com/maps/api/place/photo?maxwidth=400&photoreference={photo_ref}&key={os.getenv('GOOGLE_PLACES_API_KEY')}"
                            place_info.photos = [PlacePhoto(url=photo_url)]
                        
                        # ë² ìŠ¤íŠ¸ ë¦¬ë·° ì¶”ê°€
                        if 'reviews' in details:
                            best_review = max(details['reviews'], key=lambda x: x.get('rating', 0))
                            place_info.best_review = best_review.get('text')
                    
                    place_details.append(place_info)
                    print(f"ìž¥ì†Œ ì •ë³´ ì¶”ê°€ ì™„ë£Œ: {place_name}")
                    
                except Exception as e:
                    print(f"ìž¥ì†Œ ì •ë³´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({place_info}): {str(e)}")
                    continue
            
            return place_details
            
        except Exception as e:
            raise Exception(f"URL ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    def _process_naver_blog(self, url: str) -> List[PlaceInfo]:
        """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê¸€ì„ ì²˜ë¦¬í•˜ì—¬ ìš”ì•½ì„ ìƒì„±"""
        try:
            import requests
            from bs4 import BeautifulSoup
            
            # ë¸”ë¡œê·¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
            response = requests.get(url)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
            content = soup.get_text()
            
            # í…ìŠ¤íŠ¸ ë¶„í•  ë° ìš”ì•½
            chunks = self.text_service.split_text(content)
            summary = self.text_service.summarize_text(chunks)
            
            # ìž¥ì†Œ ì¶”ì¶œ ë° ì •ë³´ ìˆ˜ì§‘
            place_names = self.place_service.extract_place_names(summary)
            print(f"ì¶”ì¶œëœ ìž¥ì†Œ: {place_names}")
            
            # ìž¥ì†Œ ì •ë³´ ìˆ˜ì§‘
            place_details = []
            for place_name in place_names:
                try:
                    # ìž¥ì†Œëª…ê³¼ ì§€ì—­ëª… ë¶„ë¦¬
                    if " (" in place_name and ")" in place_name:
                        place_name, area = place_name.split(" (")
                        area = area.rstrip(")")
                    else:
                        place_name = place_name
                        area = "ì¼ë³¸"
                    
                    # Google Places APIë¡œ ìž¥ì†Œ ì •ë³´ ê²€ìƒ‰
                    place_info = PlaceInfo(
                        name=place_name,
                        source_url=url,
                        description=self._extract_place_description(summary, place_name),
                        google_info={}
                    )
                    
                    # Google Places API ê²€ìƒ‰ ì‹œë„
                    places_result = self.gmaps.places(f"{place_name} {area}")
                    if places_result['results']:
                        place = places_result['results'][0]
                        place_id = place['place_id']
                        details = self.gmaps.place(place_id, language='ko')['result']
                        
                        # OpenAIë¡œ ìž¥ì†Œ ì„¤ëª… ìƒì„±
                        official_description = self._get_place_description_from_openai(place_name, details.get('types', ['ì •ë³´ ì—†ìŒ'])[0])
                        if official_description:
                            place_info.official_description = official_description
                        
                        # ì¶”ê°€ ì •ë³´ ì—…ë°ì´íŠ¸
                        place_info.formatted_address = details.get('formatted_address')
                        place_info.rating = details.get('rating')
                        place_info.phone = details.get('formatted_phone_number')
                        place_info.website = details.get('website')
                        place_info.price_level = details.get('price_level')
                        place_info.opening_hours = details.get('opening_hours', {}).get('weekday_text')
                        place_info.google_info = details
                        
                        # ì‚¬ì§„ URL ì¶”ê°€
                        if 'photos' in details:
                            photo_ref = details['photos'][0]['photo_reference']
                            photo_url = f"https://maps.googleapis.com/maps/api/place/photo?maxwidth=400&photoreference={photo_ref}&key={os.getenv('GOOGLE_PLACES_API_KEY')}"
                            place_info.photos = [PlacePhoto(url=photo_url)]
                        
                        # ë² ìŠ¤íŠ¸ ë¦¬ë·° ì¶”ê°€
                        if 'reviews' in details:
                            best_review = max(details['reviews'], key=lambda x: x.get('rating', 0))
                            place_info.best_review = best_review.get('text')
                    
                    place_details.append(place_info)
                    print(f"ìž¥ì†Œ ì •ë³´ ì¶”ê°€ ì™„ë£Œ: {place_name}")
                    
                except Exception as e:
                    print(f"ìž¥ì†Œ ì •ë³´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({place_name}): {str(e)}")
                    # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ê¸°ë³¸ ì •ë³´ëŠ” ì¶”ê°€
                    place_details.append(PlaceInfo(
                        name=place_name,
                        source_url=url,
                        description=self._extract_place_description(summary, place_name),
                        google_info={}
                    ))
                    continue
            
            return place_details
            
        except Exception as e:
            raise Exception(f"URL ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    def _extract_place_description(self, summary: str, place_name: str) -> str:
        """ìš”ì•½ í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ì • ìž¥ì†Œì— ëŒ€í•œ ì„¤ëª…ì„ ì¶”ì¶œ"""
        try:
            lines = summary.split('\n')
            description = ""
            
            for i, line in enumerate(lines):
                if place_name in line:
                    # í˜„ìž¬ ì¤„ê³¼ ë‹¤ìŒ ëª‡ ì¤„ì„ í¬í•¨í•˜ì—¬ ì„¤ëª… ì¶”ì¶œ
                    description = ' '.join(lines[i:i+3])
                    break
            
            return description.strip() or "ìž¥ì†Œ ì„¤ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        except Exception as e:
            print(f"ìž¥ì†Œ ì„¤ëª… ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return "ìž¥ì†Œ ì„¤ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    def _format_final_result(self, content_infos: List[ContentInfo], place_details: List[PlaceInfo], processing_time: float, urls: List[str]) -> str:
        """ìµœì¢… ê²°ê³¼ ë¬¸ìžì—´ì„ í¬ë§·íŒ…í•˜ëŠ” ë©”ì„œë“œ"""
        
        # 1. ê¸°ë³¸ ì •ë³´ í—¤ë”
        final_result = f"""
=== ì—¬í–‰ ì •ë³´ ìš”ì•½ ===
ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ

ë¶„ì„í•œ ì˜ìƒ:
{'='*50}"""
        
        # 2. ë¹„ë””ì˜¤ ì •ë³´
        if content_infos:
            for info in content_infos:
                final_result += f"""
ì œëª©: {info.title}
ì±„ë„: {info.author}
URL: {info.url}"""
        else:
            final_result += f"\nURL: {chr(10).join(urls)}"
        
        final_result += f"\n{'='*50}\n\n=== ìž¥ì†Œë³„ ìƒì„¸ ì •ë³´ ===\n"

        # 3. ìž¥ì†Œë³„ ì •ë³´
        for idx, place in enumerate(place_details, 1):
            final_result += f"""
{idx}. {place.name}
{'='*50}

[ìœ íŠœë²„ì˜ ë¦¬ë·°]"""
            
            # ì„¤ëª…ì—ì„œ "ë°©ë¬¸í•œ ìž¥ì†Œ:" ë¶€ë¶„ ì œê±°
            description = place.description or 'ìž¥ì†Œ ì„¤ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
            if "ë°©ë¬¸í•œ ìž¥ì†Œ:" in description:
                # "ë°©ë¬¸í•œ ìž¥ì†Œ:" ì´í›„ì˜ ì²« ë²ˆì§¸ "-" ë˜ëŠ” "íƒ€ìž„ìŠ¤íƒ¬í”„:" ì´ì „ê¹Œì§€ì˜ í…ìŠ¤íŠ¸ ì œê±°
                parts = description.split(" - ", 1)
                if len(parts) > 1:
                    description = parts[1].strip()
            
            # ì„¤ëª…ê³¼ ì¶”ì²œì‚¬í•­ ë¶„ë¦¬
            if " - ì¶”ì²œ ì‚¬í•­:" in description:
                desc_parts = description.split(" - ì¶”ì²œ ì‚¬í•­:", 1)
                description = desc_parts[0].strip()
                recommendations = desc_parts[1].strip()
                final_result += f"""
ìž¥ì†Œì„¤ëª…: {description}

[ì¶”ì²œ ì‚¬í•­]
{recommendations}"""
            else:
                final_result += f"""
ìž¥ì†Œì„¤ëª…: {description}"""

            # êµ¬ê¸€ ìž¥ì†Œ ì •ë³´ê°€ ìžˆëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€
            if place.google_info:
                
                final_result += f"""

                [ìž¥ì†Œ ì„¤ëª…]
{place.official_description or 'ì„¤ëª… ì—†ìŒ'}
[êµ¬ê¸€ ìž¥ì†Œ ì •ë³´]
ìž¥ì†Œíƒ€ìž…: {place.types[0] if place.types and len(place.types) > 0 else 'ì •ë³´ ì—†ìŒ'}
ðŸ  ì£¼ì†Œ: {place.formatted_address or 'ì •ë³´ ì—†ìŒ'}
â­ í‰ì : {place.rating or 'None'}
ðŸ“ž ì „í™”: {place.phone or 'None'}
ðŸŒ ì›¹ì‚¬ì´íŠ¸: {place.website or 'None'}
ðŸ’° ê°€ê²©ëŒ€: {'â‚©' * place.price_level if place.price_level else 'ì •ë³´ ì—†ìŒ'}
â° ì˜ì—…ì‹œê°„:
{chr(10).join(place.opening_hours if place.opening_hours else ['ì •ë³´ ì—†ìŒ'])}

[ì‚¬ì§„ ë° ë¦¬ë·°]"""
                
                if place.photos:
                    for photo_idx, photo in enumerate(place.photos, 1):
                        final_result += f"""
ðŸ“¸ ì‚¬ì§„ {photo_idx}: {photo.url}"""
                
                final_result += f"""
â­ ë² ìŠ¤íŠ¸ ë¦¬ë·°: {place.best_review or 'ë¦¬ë·° ì—†ìŒ'}"""
            
            final_result += f"\n{'='*50}"
        
        return final_result

    def search_content(self, query: str) -> List[Dict]:
        """ë²¡í„° DBì—ì„œ ì½˜í…ì¸  ê²€ìƒ‰"""
        try:
            results = self.repository.query_vectordb(query)
            filtered_results = []

            for doc in results:
                if isinstance(doc, Document) and hasattr(doc, "metadata") and hasattr(doc, "page_content"):
                    filtered_results.append({
                        'content': doc.page_content,
                        'metadata': doc.metadata
                    })
                else:
                    print(f"âš ï¸ ìž˜ëª»ëœ ë°ì´í„° íƒ€ìž… ê°ì§€: {type(doc)} - {doc}")

            return filtered_results
        except Exception as e:
            raise Exception(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    @staticmethod
    def _get_youtube_transcript(video_id: str) -> str:
        """YouTube ì˜ìƒì˜ ìžë§‰ì„ ê°€ì ¸ì˜´"""
        try:
            print(f"\n=== ìžë§‰ ì¶”ì¶œ ì‹œìž‘: {video_id} ===")
            transcripts = YouTubeTranscriptApi.list_transcripts(video_id)
            
            # 1. í•œêµ­ì–´ ìžë§‰ ì‹œë„
            try:
                transcript = transcripts.find_transcript(['ko'])
                transcript_list = transcript.fetch()
                print("âœ… í•œêµ­ì–´ ìžë§‰ ì°¾ìŒ")
                
                # ìžë§‰ í…ìŠ¤íŠ¸ êµ¬ì„±
                transcript_text = []
                for entry in transcript_list:
                    timestamp = YouTubeService._format_timestamp(entry['start'])
                    text = entry['text'].strip()
                    if text:  # ë¹ˆ í…ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
                        transcript_text.append(f"[{timestamp}] {text}")
                
                result = "\n".join(transcript_text)
                print(f"ðŸ“ ì¶”ì¶œëœ í•œêµ­ì–´ ìžë§‰ ê¸¸ì´: {len(result)} ìž")
                print("=== ìžë§‰ ì¼ë¶€ ===")
                print(result[:500])  # ì²˜ìŒ 500ìžë§Œ ì¶œë ¥
                return result
                
            except (TranscriptsDisabled, NoTranscriptFound) as e:
                print(f"âš ï¸ í•œêµ­ì–´ ìžë§‰ ì—†ìŒ: {str(e)}")

            # 2. ìžë™ ìƒì„±ëœ í•œêµ­ì–´ ìžë§‰ ì‹œë„
            try:
                transcript = transcripts.find_generated_transcript(['ko'])
                transcript_list = transcript.fetch()
                print("âœ… ìžë™ ìƒì„±ëœ í•œêµ­ì–´ ìžë§‰ ì°¾ìŒ")
                
                transcript_text = []
                for entry in transcript_list:
                    timestamp = YouTubeService._format_timestamp(entry['start'])
                    text = entry['text'].strip()
                    if text:
                        transcript_text.append(f"[{timestamp}] {text}")
                
                result = "\n".join(transcript_text)
                print(f"ðŸ“ ì¶”ì¶œëœ ìžë™ ìƒì„± í•œêµ­ì–´ ìžë§‰ ê¸¸ì´: {len(result)} ìž")
                print("=== ìžë§‰ ì¼ë¶€ ===")
                print(result[:500])
                return result
                
            except Exception as e:
                print(f"âš ï¸ ìžë™ ìƒì„±ëœ í•œêµ­ì–´ ìžë§‰ ì—†ìŒ: {str(e)}")

            # 3. ì˜ì–´ ìžë§‰ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­
            try:
                transcript = transcripts.find_transcript(['en'])
                transcript_list = transcript.fetch()
                print("âœ… ì˜ì–´ ìžë§‰ ì°¾ìŒ")
                
                # ì˜ì–´ ìžë§‰ í…ìŠ¤íŠ¸ êµ¬ì„±
                en_texts = []
                timestamps = []
                for entry in transcript_list:
                    timestamp = YouTubeService._format_timestamp(entry['start'])
                    text = entry['text'].strip()
                    if text:
                        en_texts.append(text)
                        timestamps.append(timestamp)
                
                # OpenAIë¥¼ ì‚¬ìš©í•˜ì—¬ í•œêµ­ì–´ë¡œ ë²ˆì—­
                combined_text = " ".join(en_texts)
                response = openai.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "system", "content": "You are a professional translator specializing in Korean travel content."},
                        {"role": "user", "content": f"Translate the following English text to Korean, maintaining the travel-related context:\n\n{combined_text}"}
                    ],
                    temperature=0.3
                )
                
                translated_text = response.choices[0].message.content
                
                # ë²ˆì—­ëœ í…ìŠ¤íŠ¸ë¥¼ íƒ€ìž„ìŠ¤íƒ¬í”„ì™€ ê²°í•©
                result = f"[ë²ˆì—­ëœ ìžë§‰]\n"
                sentences = translated_text.split('. ')
                for i, (timestamp, sentence) in enumerate(zip(timestamps, sentences)):
                    if sentence.strip():
                        result += f"[{timestamp}] {sentence.strip()}\n"
                
                print(f"ðŸ“ ë²ˆì—­ëœ ìžë§‰ ê¸¸ì´: {len(result)} ìž")
                print("=== ë²ˆì—­ëœ ìžë§‰ ì¼ë¶€ ===")
                print(result[:500])
                return result
                
            except Exception as e:
                print(f"âš ï¸ ì˜ì–´ ìžë§‰ ë³€í™˜ ì‹¤íŒ¨: {str(e)}")

            raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ ìžë§‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        except Exception as e:
            print(f"âŒ ìžë§‰ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            raise ValueError(f"ë¹„ë””ì˜¤ {video_id}ì˜ ìžë§‰ì„ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")

    @staticmethod
    def _format_timestamp(seconds: float) -> str:
        """ì´ˆë¥¼ ì‹œ:ë¶„:ì´ˆ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        return f"{hours:02d}:{minutes:02d}:{secs:02d}"

    def _get_video_info(self, video_id: str) -> VideoInfo:
        """YouTube ë¹„ë””ì˜¤ ì •ë³´ë¥¼ ê°€ì ¸ì˜´"""
        try:
            import requests
            
            # noembed APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë””ì˜¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            api_url = f"https://noembed.com/embed?url=https://www.youtube.com/watch?v={video_id}"
            response = requests.get(api_url)
            
            if response.status_code == 200:
                data = response.json()
                return VideoInfo(
                    url=f"https://www.youtube.com/watch?v={video_id}",
                    title=data.get('title'),
                    channel=data.get('author_name')
                )
            else:
                print(f"[get_video_info] API ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {response.status_code}")
                return VideoInfo(
                    url=f"https://www.youtube.com/watch?v={video_id}",
                    title="ì œëª©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ",
                    channel="ì±„ë„ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ"
                )
            
        except Exception as e:
            print(f"ë¹„ë””ì˜¤ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
            return VideoInfo(
                url=f"https://www.youtube.com/watch?v={video_id}",
                title="ì œëª©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ",
                channel="ì±„ë„ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ"
            )

    def _get_blog_info(self, url: str) -> Dict[str, str]:
        """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜´"""
        try:
            import requests
            from bs4 import BeautifulSoup
            
            response = requests.get(url)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë¸”ë¡œê·¸ ì œëª© ì¶”ì¶œ ì‹œë„
            title = None
            title_tag = soup.find('meta', property='og:title')
            if title_tag:
                title = title_tag.get('content')
            
            if not title:
                title_tag = soup.find('title')
                if title_tag:
                    title = title_tag.text
            
            # ìž‘ì„±ìž ì •ë³´ ì¶”ì¶œ ì‹œë„
            author = None
            author_tag = soup.find('meta', property='og:article:author')
            if author_tag:
                author = author_tag.get('content')
            
            if not author:
                # ë¸”ë¡œê·¸ URLì—ì„œ ìž‘ì„±ìž ID ì¶”ì¶œ
                try:
                    blog_id = url.split('blog.naver.com/')[1].split('/')[0]
                    author = f"ë„¤ì´ë²„ ë¸”ë¡œê·¸ | {blog_id}"
                except:
                    author = "ë„¤ì´ë²„ ë¸”ë¡œê·¸ ìž‘ì„±ìž"
            
            return {
                'title': title or "ì œëª©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ",
                'author': author or "ìž‘ì„±ìž ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ"
            }
            
        except Exception as e:
            print(f"ë¸”ë¡œê·¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
            return {
                'title': "ì œëª©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ",
                'author': "ìž‘ì„±ìž ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ"
            }

    def generate_final_summary(self, content_infos: List[ContentInfo], processing_time: float, place_details: List[PlaceInfo]) -> Dict[str, str]:
        """ìµœì¢… ìš”ì•½ì„ ìƒì„±"""
        summaries = {}
        
        for content in content_infos:
            summary = f"=== ì—¬í–‰ ì •ë³´ ìš”ì•½ ===\n"
            summary += f"ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ\n\n"
            
            # ë¶„ì„í•œ ì½˜í…ì¸  ì •ë³´
            summary += "ë¶„ì„í•œ ì½˜í…ì¸ :\n"
            summary += "=" * 50 + "\n"
            for idx, info in enumerate(content_infos, 1):
                summary += f"{idx}. {info.platform.value.upper()}\n"
                summary += f"ì œëª©: {info.title}\n"
                summary += f"ìž‘ì„±ìž: {info.author}\n"
                summary += f"URL: {info.url}\n\n"
            
            summary += "=" * 50 + "\n\n"
            
            # ìž¥ì†Œë³„ ìƒì„¸ ì •ë³´
            summary += "=== ìž¥ì†Œë³„ ìƒì„¸ ì •ë³´ ===\n\n"
            
            # í˜„ìž¬ ì½˜í…ì¸ ì™€ ê´€ë ¨ëœ ìž¥ì†Œë§Œ í•„í„°ë§
            content_places = [place for place in place_details if place.source_url == content.url]
            
            # ìœ íš¨í•œ ìž¥ì†Œë§Œ í•„í„°ë§
            valid_places = []
            for place in content_places:
                # 1. ì‚¬ì§„ì´ ìžˆëŠ”ì§€ í™•ì¸
                has_photos = place.photos and len(place.photos) > 0
                
                # 2. ìœ„ë„/ê²½ë„ê°€ ìžˆëŠ”ì§€ í™•ì¸
                has_coordinates = (
                    place.geometry and 
                    place.geometry.latitude is not None and 
                    place.geometry.longitude is not None
                )
                
                # 3. ì£¼ì†Œê°€ ì¼ë³¸ì¸ì§€ í™•ì¸
                is_japan_address = (
                    place.formatted_address and 
                    ("æ—¥æœ¬" in place.formatted_address or 
                     "Japan" in place.formatted_address or 
                     "ì¼ë³¸" in place.formatted_address)
                )
                
                # ëª¨ë“  ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ê²½ìš°ë§Œ í¬í•¨
                if has_photos and has_coordinates and is_japan_address:
                    valid_places.append(place)
                else:
                    print(f"ìž¥ì†Œ ì œì™¸: {place.name}")
                    print(f"- ì‚¬ì§„ ìžˆìŒ: {has_photos}")
                    print(f"- ì¢Œí‘œ ìžˆìŒ: {has_coordinates}")
                    print(f"- ì¼ë³¸ ì£¼ì†Œ: {is_japan_address}")
            
            # ìœ íš¨í•œ ìž¥ì†Œë“¤ë§Œ ìš”ì•½ì— í¬í•¨
            for idx, place in enumerate(valid_places, 1):
                summary += f"{idx}. {place.name}\n"
                summary += "=" * 50 + "\n\n"
                
                # ìœ íŠœë²„/ë¸”ë¡œê±°ì˜ ë¦¬ë·°
                summary += "[ìœ íŠœë²„/ë¸”ë¡œê±°ì˜ ë¦¬ë·°]\n"
                summary += f"ìž¥ì†Œì„¤ëª…: {place.description or 'ìž¥ì†Œ ì„¤ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}\n\n"
                
                # êµ¬ê¸€ ìž¥ì†Œ ì •ë³´
                if place.google_info:
                    summary += "[êµ¬ê¸€ ìž¥ì†Œ ì •ë³´]\n"
                    summary += f"ðŸ  ì£¼ì†Œ: {place.formatted_address or 'ì •ë³´ ì—†ìŒ'}\n"
                    summary += f"ðŸ“ ì¢Œí‘œ: ({place.geometry.latitude}, {place.geometry.longitude})\n"
                    summary += f"â­ í‰ì : {place.rating or 'None'}\n"
                    summary += f"ðŸ“ž ì „í™”: {place.phone or 'None'}\n"
                    summary += f"ðŸŒ ì›¹ì‚¬ì´íŠ¸: {place.website or 'None'}\n"
                    summary += f"ðŸ’° ê°€ê²©ëŒ€: {'â‚©' * place.price_level if place.price_level else 'ì •ë³´ ì—†ìŒ'}\n"
                    
                    # ì˜ì—…ì‹œê°„
                    summary += "â° ì˜ì—…ì‹œê°„:\n"
                    if place.opening_hours:
                        for hours in place.opening_hours:
                            summary += f"{hours}\n"
                    else:
                        summary += "ì •ë³´ ì—†ìŒ\n"
                    
                    # ì‚¬ì§„ ë° ë¦¬ë·°
                    summary += "\n[ì‚¬ì§„ ë° ë¦¬ë·°]\n"
                    if place.photos:
                        for photo_idx, photo in enumerate(place.photos[:1], 1):
                            summary += f"ðŸ“¸ ì‚¬ì§„ {photo_idx}: {photo.url}\n"
                    summary += f"â­ ë² ìŠ¤íŠ¸ ë¦¬ë·°: {place.best_review or 'ë¦¬ë·° ì—†ìŒ'}\n"
                
                summary += "=" * 50 + "\n\n"
            
            # ìœ íš¨í•œ ìž¥ì†Œê°€ ì—†ëŠ” ê²½ìš° ë©”ì‹œì§€ ì¶”ê°€
            if not valid_places:
                summary += "â€» ìœ íš¨í•œ ìž¥ì†Œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. (ì‚¬ì§„, ì¢Œí‘œ, ì¼ë³¸ ì£¼ì†Œ ì¤‘ í•˜ë‚˜ ì´ìƒ ëˆ„ë½)\n"
            
            summaries[content.url] = summary
        
        return summaries

    def _get_place_description_from_openai(self, place_name: str, place_type: str) -> str:
        """OpenAIë¥¼ ì‚¬ìš©í•˜ì—¬ ìž¥ì†Œì— ëŒ€í•œ ì¼ë°˜ì ì¸ ì„¤ëª… ìƒì„±"""
        try:
            prompt = f"""ë‹¤ìŒ ìž¥ì†Œì— ëŒ€í•œ ì •í™•í•˜ê³  ê°„ê²°í•œ ì„¤ëª…ì„ ì œê³µí•˜ì„¸ìš”.  
ì„¤ëª…ì€ 10ìžë¡œ ì œí•œë˜ë©°, í•µì‹¬ ì •ë³´ë§Œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
ìž¥ì†Œ: {place_name}
íƒ€ìž…: {place_type}
ë°˜ë“œì‹œ ì§§ê³  ëª…í™•í•œ í•œ ë¬¸ìž¥ìœ¼ë¡œ ìž‘ì„±í•˜ì„¸ìš”."""

            response = openai.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì¼ë³¸ ì „ë¬¸ ì—¬í–‰ ê°€ì´ë“œìž…ë‹ˆë‹¤. ìž¥ì†Œì— ëŒ€í•œ ê°ê´€ì ì´ê³  ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=30
            )
            
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"ìž¥ì†Œ ì„¤ëª… ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return None

    def process_place_info(self, place_name: str, timestamp: str, description: str) -> PlaceInfo:
        """PlaceServiceì˜ process_place_infoë¥¼ í˜¸ì¶œ"""
        return self.place_service.process_place_info(place_name, timestamp, description)

class YouTubeSubtitleService:
    """YouTube ìžë§‰ ë° ë¹„ë””ì˜¤ ì •ë³´ ì²˜ë¦¬ ì„œë¹„ìŠ¤"""
    
    @staticmethod
    def get_video_info(video_url: str) -> Tuple[str, str]:
        try:
            video_id = video_url.split("v=")[-1].split("&")[0]
            api_url = f"https://noembed.com/embed?url=https://www.youtube.com/watch?v={video_id}"
            response = requests.get(api_url)
            if response.status_code == 200:
                data = response.json()
                title = data.get('title')
                author_name = data.get('author_name')
                print(f"[get_video_info] ì œëª©: {title}, ì±„ë„: {author_name}")
                return title, author_name
            print(f"[get_video_info] API ì‘ë‹µ ìƒíƒœ ì½”ë“œ: {response.status_code}")
            return None, None
        except Exception as e:
            print(f"ì˜ìƒ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
            return None, None

    @staticmethod
    def process_link(url: str) -> str:
        link_type = YouTubeSubtitleService._detect_link_type(url)
        print(f"[process_link] ë§í¬ ìœ í˜• ê°ì§€: {link_type}")
        
        if link_type == "youtube":
            text = YouTubeSubtitleService._get_youtube_transcript(url)
        elif link_type == "text_file":
            text = YouTubeSubtitleService._get_text_from_file(url)
        else:
            text = YouTubeSubtitleService._get_text_from_webpage(url)
        
        print(f"[process_link] ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}")
        return text

    @staticmethod
    def _detect_link_type(url: str) -> str:
        if "youtube.com" in url or "youtu.be" in url:
            return "youtube"
        elif url.endswith(".txt"):
            return "text_file"
        elif url.startswith("http"):
            return "webpage"
        else:
            return "unknown"

    @staticmethod
    def _format_timestamp(seconds: float) -> str:
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        return f"{hours:02d}:{minutes:02d}:{secs:02d}"

    @staticmethod
    def _get_youtube_transcript(video_url: str) -> str:
        video_id = video_url.split("v=")[-1].split("&")[0]
        print(f"[get_youtube_transcript] ë¹„ë””ì˜¤ ID: {video_id}")
        
        try:
            transcripts = YouTubeTranscriptApi.list_transcripts(video_id)
            
            # 1. ë¨¼ì € í•œêµ­ì–´ ìžë§‰ ì‹œë„
            try:
                transcript = transcripts.find_transcript(['ko'])
                transcript_text = "\n".join([f"[{YouTubeSubtitleService._format_timestamp(entry['start'])}] {entry['text']}" for entry in transcript.fetch()])
                print(f"[get_youtube_transcript] í•œêµ­ì–´ ìžë§‰ ì¶”ì¶œ ì™„ë£Œ. ê¸¸ì´: {len(transcript_text)}")
                return transcript_text
            except (TranscriptsDisabled, NoTranscriptFound):
                print("[get_youtube_transcript] í•œêµ­ì–´ ìžë§‰ ì—†ìŒ.")

            # 2. ì˜ì–´ ìžë§‰ ì‹œë„
            try:
                transcript = transcripts.find_transcript(['en'])
                transcript_text = "\n".join([f"[{YouTubeSubtitleService._format_timestamp(entry['start'])}] {entry['text']}" for entry in transcript.fetch()])
                print(f"[get_youtube_transcript] ì˜ì–´ ìžë§‰ ì¶”ì¶œ ì™„ë£Œ. ê¸¸ì´: {len(transcript_text)}")
                return transcript_text
            except (TranscriptsDisabled, NoTranscriptFound):
                print("[get_youtube_transcript] ì˜ì–´ ìžë§‰ ì—†ìŒ.")

            # 3. ì‚¬ìš© ê°€ëŠ¥í•œ ì²« ë²ˆì§¸ ìžë§‰ ì‹œë„
            try:
                transcript = transcripts.find_generated_transcript()
                transcript_text = "\n".join([f"[{YouTubeSubtitleService._format_timestamp(entry['start'])}] {entry['text']}" for entry in transcript.fetch()])
                print(f"[get_youtube_transcript] ìƒì„±ëœ ìžë§‰ ì¶”ì¶œ ì™„ë£Œ. ê¸¸ì´: {len(transcript_text)}")
                return transcript_text
            except Exception as e:
                print(f"[get_youtube_transcript] ìƒì„±ëœ ìžë§‰ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

            raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ ìžë§‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        except Exception as e:
            raise ValueError(f"ë¹„ë””ì˜¤ {video_id}ì˜ ìžë§‰ì„ ê°€ì ¸ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")

    @staticmethod
    def _get_text_from_file(url: str) -> str:
        try:
            response = requests.get(url)
            response.raise_for_status()
            text = response.text.strip()
            print(f"[get_text_from_file] í…ìŠ¤íŠ¸ íŒŒì¼ ì¶”ì¶œ ì™„ë£Œ. ê¸¸ì´: {len(text)}")
            return text
        except Exception as e:
            raise ValueError(f"í…ìŠ¤íŠ¸ íŒŒì¼ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ”ë° ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    @staticmethod
    def _get_text_from_webpage(url: str) -> str:
        try:
            response = requests.get(url)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, "html.parser")
            text = soup.get_text(separator="\n").strip()
            text = text[:10000]  # ê¸¸ì´ ì œí•œ 10000ìž
            print(f"[get_text_from_webpage] ì›¹íŽ˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ. ê¸¸ì´: {len(text)}")
            return text
        except Exception as e:
            raise ValueError(f"ì›¹íŽ˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ”ë° ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

class TextProcessingService:
    """í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì„œë¹„ìŠ¤"""
    
    @staticmethod
    def split_text(text: str, max_chunk_size: int = CHUNK_SIZE) -> List[str]:
        words = text.split()
        total_words = len(words)
        num_chunks = ceil(total_words / (max_chunk_size // 5))
        chunks = []
        for i in range(num_chunks):
            start = i * (max_chunk_size // 5)
            end = start + (max_chunk_size // 5)
            chunk = ' '.join(words[start:end])
            chunks.append(chunk)
        print(f"[split_text] ì´ ë‹¨ì–´ ìˆ˜: {total_words}, ì²­í¬ ìˆ˜: {num_chunks}")
        return chunks

    @staticmethod
    def summarize_text(transcript_chunks: List[str], model: str = MODEL) -> str:
        summaries = []
        for idx, chunk in enumerate(transcript_chunks):
            prompt = TextProcessingService._generate_prompt(chunk)
            try:
                response = openai.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": "You are a travel expert who provides detailed recommendations for places to visit, foods to eat, precautions, and suggestions based on transcripts."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.1,
                    max_tokens=1500
                )
                summary = response.choices[0].message.content
                summaries.append(summary)
                print(f"ì²­í¬ {idx+1}/{len(transcript_chunks)} ìš”ì•½ ì™„ë£Œ.")
                print(f"[ì²­í¬ {idx+1} ìš”ì•½ ë‚´ìš© ì¼ë¶€]")
                print(summary[:9500])
            except Exception as e:
                raise ValueError(f"ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        # ê°œë³„ ìš”ì•½ì„ í•©ì³ì„œ ìµœì¢… ìš”ì•½
        combined_summaries = "\n".join(summaries)
        final_prompt = f"""
ì•„ëž˜ëŠ” ì—¬ëŸ¬ ì²­í¬ë¡œ ë‚˜ë‰œ ìš”ì•½ìž…ë‹ˆë‹¤. ì´ ìš”ì•½ë“¤ì„ í†µí•©í•˜ì—¬ ë‹¤ìŒì˜ í˜•ì‹ìœ¼ë¡œ ìµœì¢… ìš”ì•½ì„ ìž‘ì„±í•´ ì£¼ì„¸ìš”. ë°˜ë“œì‹œ ì•„ëž˜ í˜•ì‹ì„ ë”°ë¥´ê³ , ë¹ ì§€ëŠ” ë‚´ìš© ì—†ì´ ëª¨ë“  ì •ë³´ë¥¼ í¬í•¨í•´ ì£¼ì„¸ìš”.
**ìš”êµ¬ ì‚¬í•­:**
1. ìž¥ì†Œ, ìŒì‹, ìœ ì˜ ì‚¬í•­, ì¶”ì²œ ì‚¬í•­ ë“± ê°ê°ì˜ ì •ë³´ë¥¼ ì„¸ë¶€ì ìœ¼ë¡œ ìž‘ì„±í•´ ì£¼ì„¸ìš”.
2. ë§Œì•½ í•´ë‹¹ ìž¥ì†Œì—ì„œ ë¨¹ì€ ìŒì‹, ìœ ì˜ ì‚¬í•­, ì¶”ì²œ ì‚¬í•­ì´ ì—†ë‹¤ë©´ ìž‘ì„±í•˜ì§€ ì•Šê³  ë„˜ì–´ê°€ë„ ë©ë‹ˆë‹¤.
3. ë°©ë¬¸í•œ ìž¥ì†Œê°€ ì—†ê±°ë‚˜ ìœ ì˜ ì‚¬í•­ë§Œ ìžˆì„ ë•Œ, ìœ ì˜ ì‚¬í•­ ì„¹ì…˜ì— ëª¨ì•„ì£¼ì„¸ìš”.
4. ì¶”ì²œ ì‚¬í•­ë§Œ ìžˆëŠ” ê²ƒë“¤ì€ ì¶”ì²œ ì‚¬í•­ ì„¹ì…˜ì— ëª¨ì•„ì£¼ì„¸ìš”.
5. ê°€ëŠ¥í•œ ìž¥ì†Œ ì´ë¦„ì„ ì•Œê³  ìžˆë‹¤ë©´ ì‹¤ì œ ì£¼ì†Œë¥¼ í¬í•¨í•´ ì£¼ì„¸ìš”.
7. ë³¸ë¬¸ ë‚´ì— ì–¸ê¸‰ëœ ëª¨ë“  ìž¥ì†Œ (ì˜ˆ: "ë„ì¿„ í•´ë¦¬í¬í„° ìŠ¤íŠœë””ì˜¤", "ë…¸ë³´ë¦¬ë² ì¸ " ë“±)ë¥¼ ë°˜ë“œì‹œ ê²°ê³¼ì— í¬í•¨ì‹œì¼œ ì£¼ì„¸ìš”.
8. ì£¼ì†Œê°€ í¬í•¨ëœ ê²½ìš° ì´ë¥¼ ì œì™¸í•˜ê³ , ì¼ë³¸ ë‚´ ìž¥ì†Œë§Œ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤. "ì•¼í‚¤í† ë¦¬ì§‘"ì´ë¼ê³ ë§Œ ì–¸ê¸‰ëœ ê²½ìš°ì—ëŠ” ì˜¤ì‚¬ì¹´ ë‚´ì˜ ì•¼í‚¤í† ë¦¬ì§‘ ì¤‘ í•˜ë‚˜ì˜ ì£¼ì†Œë¥¼ ì°¾ì•„ì„œ ì œê³µí•´ ì£¼ì„¸ìš”.
9. ì•„ì¹´íƒ€ ìƒ¤ë¸Œìƒ¤ë¸Œ"ì™€ ê°™ì´ ëª…í™•í•œ ë¸Œëžœë“œëª…ì´ ì–¸ê¸‰ë˜ì§€ ì•Šì€ ê²½ìš°, ì§€ì—­ ë‚´ ì í•©í•œ ìƒ¤ë¸Œìƒ¤ë¸Œì§‘ ì£¼ì†Œë¥¼ ì°¾ì•„ì„œ ì œê³µí•´ ì£¼ì„¸ìš”.

**ê²°ê³¼ í˜•ì‹:**

ê²°ê³¼ëŠ” ì•„ëž˜ í˜•ì‹ìœ¼ë¡œ ìž‘ì„±í•´ ì£¼ì„¸ìš”
ì•„ëž˜ëŠ” ì˜ˆì‹œìž…ë‹ˆë‹¤. 

ë°©ë¬¸í•œ ìž¥ì†Œ: ìŠ¤ë¯¸ë‹¤ íƒ€ì›Œ (ì£¼ì†Œ) íƒ€ìž„ìŠ¤íƒ¬í”„: [HH:MM:SS]
- ìž¥ì†Œì„¤ëª…: [ìœ íŠœë²„ì˜ ì„¤ëª…] ë„ì¿„ ìŠ¤ì¹´ì´íŠ¸ë¦¬ë¥¼ ëŒ€í‘œí•˜ëŠ” ëžœë“œë§ˆí¬ë¡œ, ì „ë§ëŒ€ì—ì„œ ë„ì¿„ ì‹œë‚´ë¥¼ í•œëˆˆì— ë³¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ìœ íŠœë²„ê°€ ë°©ë¬¸í–ˆì„ ë•ŒëŠ” ë‚ ì”¨ê°€ ì¢‹ì•„ì„œ í›„ì§€ì‚°ê¹Œì§€ ë³´ì˜€ê³ , ì•¼ê²½ì´ íŠ¹ížˆ ì•„ë¦„ë‹¤ì› ë‹¤ê³  í•©ë‹ˆë‹¤.
- ë¨¹ì€ ìŒì‹: ë¼ë©˜ ì´ì¹˜ëž€
    - ì„¤ëª…: ì§„í•œ êµ­ë¬¼ê³¼ ì«„ê¹ƒí•œ ë©´ë°œë¡œ ìœ ëª…í•œ ë¼ë©˜ ì²´ì¸ì ìœ¼ë¡œ, ê°œì¸ì‹¤ì—ì„œ íŽ¸ì•ˆí•˜ê²Œ ì‹ì‚¬í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
- ìœ ì˜ ì‚¬í•­: í˜¼ìž¡í•œ ì‹œê°„ëŒ€ í”¼í•˜ê¸°
    - ì„¤ëª…: ê´€ê´‘ì§€ ì£¼ë³€ì€ íŠ¹ížˆ ì£¼ë§ê³¼ íœ´ì¼ì— ë§¤ìš° í˜¼ìž¡í•  ìˆ˜ ìžˆìœ¼ë¯€ë¡œ, ê°€ëŠ¥í•œ í‰ì¼ì´ë‚˜ ì´ë¥¸ ì‹œê°„ì— ë°©ë¬¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
- ì¶”ì²œ ì‚¬í•­: ìŠ¤ì¹´ì´ íŠ¸ë¦¬ ì „ë§ëŒ€ ë°©ë¬¸
    - ì„¤ëª…: ë„ì¿„ì˜ ì•„ë¦„ë‹¤ìš´ ì•¼ê²½ì„ ê°ìƒí•  ìˆ˜ ìžˆìœ¼ë©°, ì‚¬ì§„ ì´¬ì˜ í•˜ê¸°ì— ìµœì ì˜ ìž¥ì†Œìž…ë‹ˆë‹¤.

ë°©ë¬¸í•œ ìž¥ì†Œ: ìœ ë‹ˆë²„ì…œ ìŠ¤íŠœë””ì˜¤ ì¼ë³¸ (ì£¼ì†Œ) íƒ€ìž„ìŠ¤íƒ¬í”„: [HH:MM:SS]
- ìž¥ì†Œì„¤ëª…: [ìœ íŠœë²„ì˜ ì„¤ëª…] ìœ íŠœë²„ê°€ ë°©ë¬¸í–ˆì„ ë•ŒëŠ” í‰ì¼ìž„ì—ë„ ì‚¬ëžŒì´ ë§Žì•˜ì§€ë§Œ, ì‹±ê¸€ë¼ì´ë”ë¥¼ ì´ìš©í•´ì„œ ëŒ€ê¸° ì‹œê°„ì„ ë§Žì´ ì¤„ì¼ ìˆ˜ ìžˆì—ˆìŠµë‹ˆë‹¤. íŠ¹ížˆ í•´ë¦¬í¬í„° êµ¬ì—­ì˜ ë¶„ìœ„ê¸°ê°€ ì‹¤ì œ ì˜í™”ì˜ í•œ ìž¥ë©´ì— ë“¤ì–´ì˜¨ ê²ƒ ê°™ì•˜ê³ , ë²„í„°ë§¥ì£¼ë„ ë§›ìžˆì—ˆë‹¤ê³  í•©ë‹ˆë‹¤.
- ìœ ì˜ ì‚¬í•­: ì§§ì€ ì˜· ì°©ìš© 
    - ì„¤ëª…: íŒ€ëž© í”Œëž˜ë‹›ì˜ ì¼ë¶€ êµ¬ì—­ì—ì„œëŠ” ë¬¼ì´ ë†’ê³  ê±°ìš¸ì´ ìžˆìœ¼ë¯€ë¡œ, ì§§ì€ ì˜·ì„ ìž…ëŠ” ê²ƒì´ ì¢‹ë‹¤.

**ìš”ì•½ ì²­í¬:**
{combined_summaries}

**ìµœì¢… ìš”ì•½:**
"""
        try:
            final_response = openai.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "You are an expert summary writer who strictly adheres to the provided format."},
                    {"role": "user", "content": final_prompt}
                ],
                temperature=0.1,
                max_tokens=4096
            )
            final_summary = final_response.choices[0].message.content
            print("\n[ìµœì¢… ìš”ì•½ ë‚´ìš© ì¼ë¶€]")
            print(final_summary[:1000])
            return final_summary
        except Exception as e:
            raise ValueError(f"ìµœì¢… ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    @staticmethod
    def _generate_prompt(transcript_chunk: str) -> str:
        language = detect(transcript_chunk)
        if language != 'ko':
            translation_instruction = "ì´ í…ìŠ¤íŠ¸ëŠ” í•œêµ­ì–´ê°€ ì•„ë‹™ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ ì£¼ì„¸ìš”.\n\n"
        else:
            translation_instruction = ""

        base_prompt = f"""
{translation_instruction}
ì•„ëž˜ëŠ” ì—¬í–‰ ìœ íŠœë²„ê°€ ì´¬ì˜í•œ ì˜ìƒì˜ ìžë§‰ìž…ë‹ˆë‹¤. ì´ ìžë§‰ì—ì„œ ë°©ë¬¸í•œ ìž¥ì†Œ, ë¨¹ì€ ìŒì‹, ìœ ì˜ ì‚¬í•­, ì¶”ì²œ ì‚¬í•­ì„ ë¶„ì„í•˜ì—¬ ì •ë¦¬í•´ ì£¼ì„¸ìš”.

**ìš”êµ¬ ì‚¬í•­:**
1. ìž¥ì†Œ, ìŒì‹, ìœ ì˜ ì‚¬í•­, ì¶”ì²œ ì‚¬í•­ ë“± ê°ê°ì˜ ì •ë³´ë¥¼ ì„¸ë¶€ì ìœ¼ë¡œ ìž‘ì„±í•´ ì£¼ì„¸ìš”.
2. ë§Œì•½ í•´ë‹¹ ìž¥ì†Œì—ì„œ ë¨¹ì€ ìŒì‹, ìœ ì˜ ì‚¬í•­, ì¶”ì²œ ì‚¬í•­ì´ ì—†ë‹¤ë©´ ìž‘ì„±í•˜ì§€ ì•Šê³  ë„˜ì–´ê°€ë„ ë©ë‹ˆë‹¤.
3. ë°©ë¬¸í•œ ìž¥ì†Œê°€ ì—†ê±°ë‚˜ ìœ ì˜ ì‚¬í•­ë§Œ ìžˆì„ ë•Œ, ìœ ì˜ ì‚¬í•­ ì„¹ì…˜ì— ëª¨ì•„ì£¼ì„¸ìš”.
4. ì¶”ì²œ ì‚¬í•­ë§Œ ìžˆëŠ” ê²ƒë“¤ì€ ì¶”ì²œ ì‚¬í•­ ì„¹ì…˜ì— ëª¨ì•„ì£¼ì„¸ìš”.
5. ê°€ëŠ¥í•œ ìž¥ì†Œ ì´ë¦„ì„ ì•Œê³  ìžˆë‹¤ë©´ ì‹¤ì œ ì£¼ì†Œë¥¼ í¬í•¨í•´ ì£¼ì„¸ìš”.
6. ìž¥ì†Œ ì„¤ëª…ì€ ë°˜ë“œì‹œ ìœ íŠœë²„ê°€ ì‹¤ì œë¡œ ì–¸ê¸‰í•œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìž‘ì„±í•´ ì£¼ì„¸ìš”.
7. ëª¨ë“  ìž¥ì†ŒëŠ” ë°˜ë“œì‹œ êµ¬ì²´ì ì¸ ì§€ì—­ëª…(ì˜ˆ: ë„ì¿„, ì˜¤ì‚¬ì¹´, êµí†  ë“±)ê³¼ í•¨ê»˜ í‘œì‹œí•´ì£¼ì„¸ìš”.
   - ì§€ì—­ëª…ì„ ì•Œ ìˆ˜ ìžˆëŠ” ê²½ìš°: "ìŠ¤ì¹´ì´íŠ¸ë¦¬ (ë„ì¿„)"
   - ì§€ì—­ëª…ì„ ì•Œ ìˆ˜ ì—†ëŠ” ê²½ìš°: "ìŠ¤ì¹´ì´íŠ¸ë¦¬ (ì¼ë³¸)"

**ê²°ê³¼ í˜•ì‹:**
ë°©ë¬¸í•œ ìž¥ì†Œ: [ìž¥ì†Œëª…] ([ì§€ì—­ëª…]) íƒ€ìž„ìŠ¤íƒ¬í”„: [HH:MM:SS]
...
"""
        print("\n[generate_prompt] ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ ì¼ë¶€:")
        print(base_prompt[:500])
        return base_prompt

class PlaceService:
    """ìž¥ì†Œ ì •ë³´ ì²˜ë¦¬ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.video_url = None

    def set_video_url(self, url: str):
        self.video_url = url

    def extract_place_names(self, summary: str) -> List[str]:
        """ìš”ì•½ í…ìŠ¤íŠ¸ì—ì„œ ìž¥ì†Œ ì´ë¦„ì„ ì¶”ì¶œ"""
        place_names = set()  # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ set ì‚¬ìš©
        
        # ëª¨ë“  ì²­í¬ì˜ ìš”ì•½ì—ì„œ ìž¥ì†Œ ì¶”ì¶œ
        chunks = summary.split("ë°©ë¬¸í•œ ìž¥ì†Œ:")
        for chunk in chunks[1:]:  # ì²« ë²ˆì§¸ëŠ” ê±´ë„ˆë›°ê¸°
            try:
                place_name = chunk.split("(")[0].strip()
                if place_name:
                    place_names.add(place_name)
                    print(f"ìž¥ì†Œ ì¶”ì¶œ: {place_name}")
            except Exception as e:
                print(f"ìž¥ì†Œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
                continue
        
        result = list(place_names)
        print(f"ì´ ì¶”ì¶œëœ ìž¥ì†Œ ëª©ë¡: {result}")
        return result

    @staticmethod
    def search_place_details(place_name: str, area: str = None) -> Dict[str, Any]:
        """Google Places APIë¥¼ ì‚¬ìš©í•˜ì—¬ ìž¥ì†Œ ì •ë³´ë¥¼ ê²€ìƒ‰"""
        try:
            gmaps = googlemaps.Client(key=os.getenv("GOOGLE_PLACES_API_KEY"))
            
            # ì§€ì—­ëª…ì´ ìžˆìœ¼ë©´ ìž¥ì†Œëª…ê³¼ í•¨ê»˜ ê²€ìƒ‰, ì—†ìœ¼ë©´ 'ì¼ë³¸'ì„ ì¶”ê°€
            search_query = f"{place_name} {area if area else 'ì¼ë³¸'}"
            print(f"[search_place_details] ê²€ìƒ‰ì–´: {search_query}")
            
            # ìž¥ì†Œ ê²€ìƒ‰
            places_result = gmaps.places(search_query)
            
            if not places_result['results']:
                print(f"[search_place_details] ìž¥ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {search_query}")
                return None
                
            place = places_result['results'][0]
            place_id = place['place_id']
            
            # ìƒì„¸ ì •ë³´ ê²€ìƒ‰
            details_result = gmaps.place(place_id, language='ko')
            if not details_result.get('result'):
                return None
                
            details = details_result['result']
            
            # ë¦¬ë·° ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            reviews = details.get('reviews', [])
            best_review = reviews[0]['text'] if reviews else None
            
            # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ìƒì„±
            return {
                'name': details.get('name', ''),
                'formatted_address': details.get('formatted_address', ''),
                'rating': details.get('rating'),
                'formatted_phone_number': details.get('formatted_phone_number', ''),
                'website': details.get('website', ''),
                'price_level': details.get('price_level'),
                'opening_hours': details.get('opening_hours', {}).get('weekday_text', []),
                'photos': details.get('photos', []),
                'best_review': best_review
            }
            
        except Exception as e:
            print(f"[search_place_details] ìž¥ì†Œ ì •ë³´ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({search_query}): {str(e)}")
            return None

    @staticmethod
    def get_place_photo_google(place_name: str) -> str:
        """Google Places APIë¥¼ ì‚¬ìš©í•˜ì—¬ ìž¥ì†Œ ì‚¬ì§„ URLì„ ê°€ì ¸ì˜´"""
        try:
            gmaps = googlemaps.Client(key=os.getenv("GOOGLE_PLACES_API_KEY"))
            places_result = gmaps.places(place_name)
            
            if not places_result['results']:
                print(f"[get_place_photo_google] ì‚¬ì§„ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {place_name}")
                return None
                
            place = places_result['results'][0]
            if not place.get('photos'):
                return None
                
            photo_reference = place['photos'][0]['photo_reference']
            photo_url = f"https://maps.googleapis.com/maps/api/place/photo?maxwidth=400&photoreference={photo_reference}&key={os.getenv('GOOGLE_PLACES_API_KEY')}"
            
            print(f"[get_place_photo_google] ì‚¬ì§„ URL ìƒì„± ì™„ë£Œ: {photo_url}")
            return photo_url
            
        except Exception as e:
            print(f"[get_place_photo_google] ì‚¬ì§„ URL ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return None

    def process_place_info(self, place_name: str, timestamp: str, description: str) -> PlaceInfo:
        """ìž¥ì†Œ ì •ë³´ë¥¼ ì²˜ë¦¬í•˜ê³  PlaceInfo ê°ì²´ë¥¼ ë°˜í™˜"""
        try:
            # Google Places APIë¡œ ìž¥ì†Œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            gmaps = googlemaps.Client(key=os.getenv("GOOGLE_PLACES_API_KEY"))
            places_result = gmaps.places(place_name)
            
            if not places_result['results']:
                return None
            
            google_place_info = places_result['results'][0]
            
            # ì‚¬ì§„ URL ê°€ì ¸ì˜¤ê¸°
            photo_url = self.get_place_photo_google(place_name)
            
            # ìž¥ì†Œ íƒ€ìž… í™•ì¸
            place_type = google_place_info.get('types', ['unknown'])[0]
            
            # OpenAIë¡œ ê³µì‹ ì„¤ëª… ìƒì„±
            official_description = self._get_place_description_from_openai(place_name, place_type)
            
            # ì˜ì—…ì‹œê°„ í¬ë§·íŒ…
            opening_hours = None
            if google_place_info.get('opening_hours'):
                opening_hours = google_place_info['opening_hours'].get('weekday_text')

            # PlaceInfo ê°ì²´ ìƒì„±
            place_info = PlaceInfo(
                name=place_name,
                source_url=self.video_url,
                timestamp=timestamp,
                description=description,
                official_description=official_description,
                formatted_address=google_place_info.get('formatted_address'),
                coordinates={
                    'lat': google_place_info['geometry']['location']['lat'],
                    'lng': google_place_info['geometry']['location']['lng']
                } if 'geometry' in google_place_info else None,
                rating=google_place_info.get('rating'),
                phone=google_place_info.get('formatted_phone_number'),
                website=google_place_info.get('website'),
                price_level=google_place_info.get('price_level'),
                opening_hours=opening_hours,
                photos=[PlacePhoto(url=photo_url)] if photo_url else None,
                best_review=google_place_info.get('reviews', [{}])[0].get('text') if google_place_info.get('reviews') else None,
                google_info=google_place_info,
                types=google_place_info.get('types')
            )
            
            return place_info
            
        except Exception as e:
            print(f"ìž¥ì†Œ ì •ë³´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return None
